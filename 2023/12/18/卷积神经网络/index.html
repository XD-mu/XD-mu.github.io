<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xd-mu.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1.Lenet和现代经典卷积神经网络 ​ 手敲一遍过去三四十年的传统神经网络，加深一下对于网络演变的认识，给自己后续学习新进的网络结构打好基础。 ​ 主要是分为：  传统卷积神经网络：LeNet 现代卷积神经网络：AlexNet、VGG、NiN、GoogleLeNet、Resnet、DenseNet">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积神经网络">
<meta property="og:url" content="https://xd-mu.github.io/2023/12/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="野生调参侠的博客">
<meta property="og:description" content="1.Lenet和现代经典卷积神经网络 ​ 手敲一遍过去三四十年的传统神经网络，加深一下对于网络演变的认识，给自己后续学习新进的网络结构打好基础。 ​ 主要是分为：  传统卷积神经网络：LeNet 现代卷积神经网络：AlexNet、VGG、NiN、GoogleLeNet、Resnet、DenseNet">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/SemBQUgKGyMF4a7.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/cezshvg3lxo9pIm.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/2agLNYQtWBUMC5u.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/VbFmuHKinsCaTz3.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/Utl14NXaMpoKuJF.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/7rjfqoUiyPaGBXg.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/2jlavDVtJOIquZk.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/wPA1OCFqsxzH9ek.png">
<meta property="og:image" content="https://s2.loli.net/2023/12/18/xsrGgqFXDu2lU8K.png">
<meta property="article:published_time" content="2023-12-18T07:20:47.000Z">
<meta property="article:modified_time" content="2023-12-18T11:54:12.192Z">
<meta property="article:author" content="野生调参侠">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/12/18/SemBQUgKGyMF4a7.png">

<link rel="canonical" href="https://xd-mu.github.io/2023/12/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>卷积神经网络 | 野生调参侠的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">野生调参侠的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xd-mu.github.io/2023/12/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/%E5%A4%B4%E5%83%8F.jpg">
      <meta itemprop="name" content="野生调参侠">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="野生调参侠的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          卷积神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-12-18 15:20:47 / 修改时间：19:54:12" itemprop="dateCreated datePublished" datetime="2023-12-18T15:20:47+08:00">2023-12-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BE%E5%86%85%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">课内学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BE%E5%86%85%E5%AD%A6%E4%B9%A0/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">传统算法学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AF%BE%E5%86%85%E5%AD%A6%E4%B9%A0/%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/%E6%9D%8E%E6%B2%90%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B/" itemprop="url" rel="index"><span itemprop="name">李沐《动手学深度学习》</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="lenet和现代经典卷积神经网络">1.Lenet和现代经典卷积神经网络</h2>
<p>​
手敲一遍过去三四十年的传统神经网络，加深一下对于网络演变的认识，给自己后续学习新进的网络结构打好基础。</p>
<p>​ 主要是分为：</p>
<ul>
<li>传统卷积神经网络：LeNet</li>
<li>现代卷积神经网络：AlexNet、VGG、NiN、GoogleLeNet、Resnet、DenseNet</li>
</ul>
<span id="more"></span>
<h2 id="传统卷积神经网络">2.传统卷积神经网络</h2>
<h3 id="lenet">2.1LeNet</h3>
<p><img src="https://s2.loli.net/2023/12/18/SemBQUgKGyMF4a7.png"></p>
<p>下面是简化版的LeNet</p>
<p><img src="https://s2.loli.net/2023/12/18/cezshvg3lxo9pIm.png" alt="image-20231218155213441" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,kernel_size=<span class="number">5</span>),nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>),nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>,<span class="number">84</span>),nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>设置输入数据并输出每一层的形状，以此来检查模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>),dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">'output shape: \t'</span>,X.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Conv2d output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sigmoid output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">AvgPool2d output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Conv2d output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">Sigmoid output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">AvgPool2d output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">400</span>])</span><br><span class="line">Linear output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Sigmoid output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">Linear output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Sigmoid output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">Linear output shape: 	 torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>​
在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。第一个卷积层使用2个像素的填充，来补偿<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2222.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path></g></g></g></svg></mjx-container></span>卷积核导致的特征减少。相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。同时，每个汇聚层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。</p>
<p>​ 训练评估Lenet-5模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">lr,num_epochs = <span class="number">0.9</span>,<span class="number">10</span></span><br><span class="line">d2l.train_ch6(net,test_iter,test_iter,num_epochs,lr,d2l.try_gpu())<span class="comment">#使用GPU进行训练</span></span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2023/12/18/2agLNYQtWBUMC5u.png" alt="image-20231218160811454" style="zoom: 80%;"></p>
<p><strong>总结：</strong></p>
<ul>
<li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li>
<li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li>
</ul>
<h2 id="现代经典卷积神经网络">3.现代经典卷积神经网络</h2>
<h3 id="alexnet">3.1AlexNet</h3>
<p><img src="https://s2.loli.net/2023/12/18/VbFmuHKinsCaTz3.png"></p>
<p><strong>模型设计：</strong></p>
<p>​
因为ImageNet大多数图像的宽高比MINIST多十倍，因此需要更大的卷积窗口来捕获特征，所以第一层卷积窗口形状是11X11。第二层中的卷积窗口形状被缩减为5
× 5，然后是3 × 3。此
外，在第一层、第二层和第五层卷积层之后，加入窗口形状为3 ×
3、步幅为2的最大汇聚层。而且，AlexNet的 卷积通道数目是LeNet的10倍。</p>
<p>​
和LeNet还有一点不同的是，AlexNet使用RELU激活函数替换掉了原来的Sigmoid激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">96</span>,kernel_size=<span class="number">11</span>,stride=<span class="number">4</span>,padding=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    </span><br><span class="line">    nn.Conv2d(<span class="number">96</span>,<span class="number">256</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    </span><br><span class="line">    nn.Conv2d(<span class="number">256</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    </span><br><span class="line">    nn.Linear(<span class="number">6400</span>,<span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">    nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    </span><br><span class="line">    nn.Linear(<span class="number">4096</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">'output shape\t'</span>,X.shape)</span><br></pre></td></tr></table></figure>
<p>输出网络形状变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Conv2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">MaxPool2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">Conv2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">MaxPool2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">Conv2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">MaxPool2d output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">Flatten output shape	 torch.Size([<span class="number">1</span>, <span class="number">6400</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><strong>训练：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=<span class="number">224</span>)</span><br><span class="line">lr,num_epochs = <span class="number">0.01</span>,<span class="number">10</span></span><br><span class="line">d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<p><img src="https://s2.loli.net/2023/12/18/Utl14NXaMpoKuJF.png" alt="image-20231218172014750" style="zoom:67%;"></p>
<h3 id="vgg">3.2VGG</h3>
<p>本质上是块的累积，更大更深的AlexNet</p>
<p><img src="https://s2.loli.net/2023/12/18/7rjfqoUiyPaGBXg.png"></p>
<p><img src="https://s2.loli.net/2023/12/18/2jlavDVtJOIquZk.png"></p>
<p>说明：</p>
<ul>
<li><p>白色矩形框：代表卷积和激活函数</p></li>
<li><p>红色矩形框：代表最大池化下载量</p></li>
<li><p>蓝色矩形框：全连接层和激活函数</p></li>
<li><p>橙色矩形框：softmax处理</p></li>
</ul>
<p>结构过程：（配置表和结构图一起观察）</p>
<p>​
1、首先输入一张224<em>224</em>3大小的图像，经过两个3<em>3的卷积层之后，所得到的特征图大小为224</em>224*64（尺寸大小不变，因为采用的是64个卷积核，所以深度也为64）。</p>
<p>​
2、通过一个最大池化下载量层，得到的特征图为112<em>112</em>64（大小缩小一半，不改变深度）。</p>
<p>​
3、再通过两个3<em>3</em>128的卷积层，得到的特征图为112<em>112</em>128（深度变为128）。</p>
<p>​
4、通过一个最大池化下载量层，得到的特征图为56<em>56</em>128（大小缩小一半，不改变深度）。</p>
<p>​
5、再通过三个3<em>3</em>256的卷积层，得到的特征图为56<em>56</em>256（深度变为256）。</p>
<p>​
6、通过一个最大池化下载量层，得到的特征图为28<em>28</em>256（大小缩小一半，不改变深度）。</p>
<p>​
7、再通过三个3<em>3</em>512的卷积层，得到的特征图为28<em>28</em>512（深度变为512）。</p>
<p>​
8、通过一个最大池化下载量层，得到的特征图为14<em>14</em>512（大小缩小一半，不改变深度）。</p>
<p>​
9、再通过三个3<em>3</em>512的卷积层，得到的特征图为14<em>14</em>512（深度变为512）。</p>
<p>​
10、通过一个最大池化下载量层，得到的特征图为7<em>7</em>512（大小缩小一半，不改变深度）。</p>
<p>​
11、再通过两个为4000个节点的全连接层以及激活函数，得到1<em>1</em>4096向量</p>
<p>​
12、再通过一个为1000个节点的全连接层（因为1000个类别），注意不需要激活函数，得到1<em>1</em>1000向量。</p>
<p>​
13、最后将通过全连接层得到的一维向量，输入到softmax激活函数，将预测结果转化为概率分布。</p>
<p><strong>模型设计：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels,out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="comment">#给一个VGG架构</span></span><br><span class="line">conv_arch = ((<span class="number">1</span>,<span class="number">64</span>),(<span class="number">1</span>,<span class="number">128</span>),(<span class="number">2</span>,<span class="number">256</span>),(<span class="number">2</span>,<span class="number">512</span>),(<span class="number">2</span>,<span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#实现VGG-11</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs,in_channels,out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks,nn.Flatten(),</span><br><span class="line">        <span class="comment">#全连接部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>),nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment">#实现网络</span></span><br><span class="line">net = vgg(conv_arch)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建输入样本并观察输入网络每层形状</span></span><br><span class="line">X = torch.randn(size=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__,<span class="string">'output shape\t'</span>,X.shape)</span><br></pre></td></tr></table></figure>
<p>模型形状输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Sequential output shape	 torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">112</span>, <span class="number">112</span>])</span><br><span class="line">Sequential output shape	 torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">Sequential output shape	 torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">Sequential output shape	 torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">Sequential output shape	 torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">Flatten output shape	 torch.Size([<span class="number">1</span>, <span class="number">25088</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">ReLU output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Dropout output shape	 torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">Linear output shape	 torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p><strong>训练模型：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">lr, num_epochs,batch_size = <span class="number">0.05</span>,<span class="number">10</span>,<span class="number">128</span></span><br><span class="line">train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size,resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<p><img src="https://s2.loli.net/2023/12/18/wPA1OCFqsxzH9ek.png"></p>
<h3 id="nin">3.3NiN</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels,out_channels,kernel_size,strides,padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels,out_channels,kernel_size=<span class="number">1</span>),nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>,<span class="number">96</span>,kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>,padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>,<span class="number">256</span>,kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>,<span class="number">10</span>,kernel_size=<span class="number">3</span>,strides=<span class="number">1</span>,padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">    nn.Flatten()</span><br><span class="line">)</span><br><span class="line"><span class="comment">#我们创建一个数据样本来查看每个块的输出形状</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">'output shape:\t'</span>, X.shape)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://s2.loli.net/2023/12/18/xsrGgqFXDu2lU8K.png" alt="image-20231218195322673">
<figcaption aria-hidden="true">image-20231218195322673</figcaption>
</figure>
<p><strong>总结：</strong></p>
<ul>
<li>NiN使用由一个卷积层和多个<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 2222.4 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></span>卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li>
<li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li>
<li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li>
<li>NiN的设计影响了许多后续卷积神经网络的设计。</li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="野生调参侠 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>野生调参侠
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://xd-mu.github.io/2023/12/18/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="卷积神经网络">https://xd-mu.github.io/2023/12/18/卷积神经网络/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/17/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" rel="prev" title="多层感知机">
      <i class="fa fa-chevron-left"></i> 多层感知机
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#lenet%E5%92%8C%E7%8E%B0%E4%BB%A3%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">1.Lenet和现代经典卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">2.传统卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lenet"><span class="nav-number">2.1.</span> <span class="nav-text">2.1LeNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%8E%B0%E4%BB%A3%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">3.</span> <span class="nav-text">3.现代经典卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#alexnet"><span class="nav-number">3.1.</span> <span class="nav-text">3.1AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#vgg"><span class="nav-number">3.2.</span> <span class="nav-text">3.2VGG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nin"><span class="nav-number">3.3.</span> <span class="nav-text">3.3NiN</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="野生调参侠"
      src="/images/%E5%A4%B4%E5%83%8F.jpg">
  <p class="site-author-name" itemprop="name">野生调参侠</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/XD-mu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;XD-mu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/2410275787@qq.com" title="E-Mail → 2410275787@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">野生调参侠</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">99k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">2:59</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>-->

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("10/06/2023 11:48:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

  
  
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<script type="text/javascript" src="/js/clicklove.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":2,"width":200,"height":400,"position":"right","hOffset":20,"vOffset":0},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
