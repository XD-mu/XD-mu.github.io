<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2023-12-4：Convolution model - Step by Step and Application</title>
      <link href="/2023/12/03/2023-12-4%EF%BC%9AConvolution-model-Step-by-Step-and-Application/"/>
      <url>/2023/12/03/2023-12-4%EF%BC%9AConvolution-model-Step-by-Step-and-Application/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1.任务"></a>1.任务</h2><p>​分步构建卷积神经网络并进行部署应用。</p><span id="more"></span><h2 id="2-Convolutional-Neural-Networks-Step-by-Step"><a href="#2-Convolutional-Neural-Networks-Step-by-Step" class="headerlink" title="2.Convolutional Neural Networks: Step by Step"></a>2.<strong>Convolutional Neural Networks: Step by Step</strong></h2><p><strong>符号说明</strong>:</p><ul><li><p>上标 $[l]$ 表示第 $l^{th}$ 层的对象。</p><ul><li>示例：$a^{[4]}$ 是第 $4^{th}$ 层的激活。$W^{[5]}$ 和 $b^{[5]}$ 是第 $5^{th}$ 层的参数。</li></ul></li><li><p>上标 $(i)$ 表示来自第 $i^{th}$ 个示例的对象。</p><ul><li>示例：$x^{(i)}$ 是第 $i^{th}$ 个训练示例输入。</li></ul></li><li><p>下标 $i$ 表示向量的第 $i^{th}$ 个条目。</p><ul><li>示例：$a^{[l]}_i$ 表示第 $l$ 层激活中的第 $i^{th}$ 个条目，假设这是一个全连接（FC）层。</li></ul></li><li><p>$n_H$、$n_W$ 和 $n_C$ 分别表示给定层的<strong>高度、宽度和通道数</strong>。如果你想引用特定的层 $l$，你也可以写成 $n_H^{[l]}$、$n_W^{[l]}$、$n_C^{[l]}$。</p></li><li><p>$n_{H_{prev}}$、$n_{W_{prev}}$ 和 $n_{C_{prev}}$ 分别表示前一层的高度、宽度和通道数。如果引用特定层 $l$，这也可以表示为 $n_H^{[l-1]}$、$n_W^{[l-1]}$、$n_C^{[l-1]}$。</p></li></ul><hr><p><strong>卷积神经网络的构建块要实现的每个函数的说明步骤：</strong></p><ul><li>卷积函数，包括：<ul><li>零填充</li><li>卷积窗口</li><li>卷积前向传播</li><li>卷积反向传播（可选）</li></ul></li><li>池化函数，包括：<ul><li>池化前向传播</li><li>创建掩码</li><li>分配值</li><li>池化反向传播（可选）</li></ul></li></ul><h3 id="2-1导入库"><a href="#2-1导入库" class="headerlink" title="2.1导入库"></a>2.1导入库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2构建网络块"><a href="#2-2构建网络块" class="headerlink" title="2.2构建网络块"></a>2.2构建网络块</h3><h4 id="2-2-1Zero-Padding（零填充）"><a href="#2-2-1Zero-Padding（零填充）" class="headerlink" title="2.2.1Zero-Padding（零填充）"></a>2.2.1Zero-Padding（零填充）</h4><img src="https://s2.loli.net/2023/12/04/1zNT4wxXM5k8nJB.png" alt="image-20231204095707437" style="zoom: 80%;" /><p><strong>主要好处如下：</strong></p><ul><li><p>它允许你使用卷积层而不必缩小体积的高度和宽度。这对于构建更深的网络非常重要，因为否则在进入更深层次时，高度&#x2F;宽度会缩小。一个重要的特例是“相同”卷积，在这种卷积中，经过一层处理后，高度&#x2F;宽度被完全保留。</p></li><li><p>它帮助我们<strong>保留了图像边缘处更多的信息</strong>。如果没有填充，图像边缘的像素会对下一层的很少数值产生影响。</p></li></ul><p><strong>代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评分函数：zero_pad</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zero_pad</span>(<span class="params">X, pad</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在数据集 X 的所有图像上用零进行填充。填充应用于图像的高度和宽度，</span></span><br><span class="line"><span class="string">    如图 1 所示。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 代表 m 张图像的一批的 python numpy 数组，形状为 (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    pad -- 整数，每个图像在垂直和水平维度上的填充量</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    X_pad -- 填充后的图像，形状为 (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ### (≈ 1 行)</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>), (pad,pad), (pad,pad), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">&quot;constant&quot;</span>)</span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x_pad = zero_pad(x, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x.shape =&quot;</span>, x.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x_pad.shape =&quot;</span>, x_pad.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x[1,1] =&quot;</span>, x[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x_pad[1,1] =&quot;</span>, x_pad[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">axarr[<span class="number">0</span>].set_title(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">axarr[<span class="number">0</span>].imshow(x[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;x_pad&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].imshow(x_pad[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x.shape = (<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x_pad.shape = (<span class="number">4</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">2</span>)</span><br><span class="line">x[<span class="number">1</span>,<span class="number">1</span>] = [[ <span class="number">0.90085595</span> -<span class="number">0.68372786</span>]</span><br><span class="line">         [-<span class="number">0.12289023</span> -<span class="number">0.93576943</span>]</span><br><span class="line">         [-<span class="number">0.26788808</span>  <span class="number">0.53035547</span>]]</span><br><span class="line">x_pad[<span class="number">1</span>,<span class="number">1</span>] = [[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">             [<span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/12/04/OEaDQUZpKiW8VFe.png" alt="image-20231204100349995"></p><h4 id="2-2-2Convolve-window（卷积窗口）"><a href="#2-2-2Convolve-window（卷积窗口）" class="headerlink" title="2.2.2Convolve window（卷积窗口）"></a>2.2.2Convolve window（卷积窗口）</h4><img src="https://s2.loli.net/2023/12/04/lVQEp8UjGea5NPn.gif" alt="Convolution_schematic" style="zoom:86%;" /><p><strong>代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评分函数：conv_single_step</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_single_step</span>(<span class="params">a_slice_prev, W, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在前一层的输出激活的一个切片 (a_slice_prev) 上应用由参数 W 定义的一个过滤器。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    a_slice_prev -- 输入数据的切片，形状为 (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- 窗口中包含的权重参数 - 形状为 (f, f, n_C_prev) 的矩阵</span></span><br><span class="line"><span class="string">    b -- 窗口中包含的偏置参数 - 形状为 (1, 1, 1) 的矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    Z -- 一个标量值，输入数据的一个切片 x 上卷积滑动窗口 (W, b) 的结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### 开始编写代码 ### (≈ 2 行代码)</span></span><br><span class="line">    <span class="comment"># a_slice 和 W 之间的元素级别乘积。暂时不要添加偏置。</span></span><br><span class="line">    s = np.multiply(a_slice_prev, W)</span><br><span class="line">    <span class="comment"># 对体积 s 的所有条目求和。</span></span><br><span class="line">    Z = np.<span class="built_in">sum</span>(s)</span><br><span class="line">    <span class="comment"># 将偏置 b 添加到 Z 上。将 b 转换为 float()，使 Z 结果为一个标量值。</span></span><br><span class="line">    Z = Z + <span class="built_in">float</span>(b)</span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">a_slice_prev = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Z = conv_single_step(a_slice_prev, W, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Z =&quot;</span>, Z)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = -<span class="number">6.999089450680221</span></span><br></pre></td></tr></table></figure><h4 id="2-2-3Convolution-forward（卷积前向传播）"><a href="#2-2-3Convolution-forward（卷积前向传播）" class="headerlink" title="2.2.3Convolution forward（卷积前向传播）"></a>2.2.3Convolution forward（卷积前向传播）</h4><img src="https://s2.loli.net/2023/12/04/EHqhAgBPZQkexcN.png" alt="vert_horiz_kiank" style="zoom:50%;" /><p>​上图只显示单个通道，使用垂直和水平起止（使用 2x2 滤波器）定义切片 </p><p>​卷积的输出形状与输入形状相关的公式是：<br>$$ n_H &#x3D; \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$<br>$$ n_W &#x3D; \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1 $$<br>$$ n_C &#x3D; \text{卷积中使用的滤波器数量}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评分函数：conv_forward</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_forward</span>(<span class="params">A_prev, W, b, hparameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现卷积函数的前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    A_prev -- 前一层的输出激活，形状为 (m, n_H_prev, n_W_prev, n_C_prev) 的 numpy 数组</span></span><br><span class="line"><span class="string">    W -- 权重，形状为 (f, f, n_C_prev, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    b -- 偏置，形状为 (1, 1, 1, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    hparameters -- 包含 &quot;stride&quot; 和 &quot;pad&quot; 的 python 字典</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    Z -- 卷积输出，形状为 (m, n_H, n_W, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    cache -- conv_backward() 函数所需的值的缓存</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ###</span></span><br><span class="line">    <span class="comment"># 从 A_prev 的形状中检索维度 (≈1 行)</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 W 的形状中检索维度 (≈1 行)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 &quot;hparameters&quot; 中检索信息 (≈2 行)</span></span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    pad = hparameters[<span class="string">&quot;pad&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用上面给出的公式计算 CONV 输出体积的维度。提示：使用 int() 进行向下取整。 (≈2 行)</span></span><br><span class="line">    n_H = (n_H_prev - f + <span class="number">2</span> * pad) // stride + <span class="number">1</span></span><br><span class="line">    n_W = (n_W_prev - f + <span class="number">2</span> * pad) // stride + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用零初始化输出体积 Z。 (≈1 行)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 通过填充 A_prev 来创建 A_prev_pad</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                               <span class="comment"># 遍历训练样本批次</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i, :, :, :]          <span class="comment"># 选择第 i 个训练样本的填充激活</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                         <span class="comment"># 遍历输出体积的垂直轴</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):                     <span class="comment"># 遍历输出体积的水平轴</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):                 <span class="comment"># 遍历输出体积的通道（= #过滤器）</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到当前“切片”的四个角 (≈4 行)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + stride</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + stride</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 使用角落来定义 a_prev_pad 的（3D）切片 (参见单元格上方的提示)。 (≈1 行)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end, : ]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 用正确的过滤器 W 和偏置 b 对（3D）切片进行卷积，以得到一个输出神经元。 (≈1 行)</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])</span><br><span class="line">                                        </span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保输出形状是正确的</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在 &quot;cache&quot; 中保存反向传播所需的信息</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成样例数据</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">10</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;pad&quot;</span> : <span class="number">2</span>,</span><br><span class="line">               <span class="string">&quot;stride&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="comment">#部署</span></span><br><span class="line">Z, cache_conv = conv_forward(A_prev, W, b, hparameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Z&#x27;s mean =&quot;</span>, np.mean(Z))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Z[3,2,1] =&quot;</span>, Z[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;cache_conv[0][1][2][3] =&quot;</span>, cache_conv[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z<span class="string">&#x27;s mean = 0.048995203528855794</span></span><br><span class="line"><span class="string">Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437</span></span><br><span class="line"><span class="string">  5.18531798  8.75898442]</span></span><br><span class="line"><span class="string">cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]</span></span><br></pre></td></tr></table></figure><h4 id="2-2-4Convolution-backward（卷积反向传播）"><a href="#2-2-4Convolution-backward（卷积反向传播）" class="headerlink" title="2.2.4Convolution backward（卷积反向传播）"></a>2.2.4Convolution backward（卷积反向传播）</h4><ul><li><strong>计算dA</strong></li></ul><p>​这是计算相对于某一特定滤波器 $W_c$ 和一个给定训练样本的成本的 $dA$ 的公式：</p><p>$$ dA +&#x3D; \sum <em>{h&#x3D;0} ^{n_H} \sum</em>{w&#x3D;0} ^{n_W} W_c \times dZ_{hw} \tag{1}$$</p><p>​其中 $W_c$ 是一个滤波器，$dZ_{hw}$ 是一个标量，对应于卷积层 Z 的输出在第 h 行和第 w 列的成本梯度（对应于在第 i 个步长向左和第 j 个步长向下时取的点积）。注意，每次我们都用相同的滤波器 $W_c$ 乘以不同的 dZ 来更新 dA。这主要是因为在计算前向传播时，每个滤波器都由不同的 a_slice 点乘并求和。因此，在计算 dA 的反向传播时，我们只是添加所有 a_slices 的梯度。</p><p>​在代码中，放在适当的 for 循环内，这个公式转化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><ul><li><strong>计算dW</strong></li></ul><p>​这是计算相对于损失的 $dW_c$（$dW_c$ 是一个滤波器的导数）的公式：</p><p>$$ dW_c  +&#x3D; \sum <em>{h&#x3D;0} ^{n_H} \sum</em>{w&#x3D;0} ^ {n_W} a_{slice} \times dZ_{hw}  \tag{2}$$</p><p>​其中 $a_{slice}$ 对应于用于生成激活 $Z_{ij}$ 的切片。因此，这最终给我们提供了关于该切片的 $W$ 的梯度。由于是相同的 $W$，我们将简单地将所有这些梯度相加，以获得 $dW$。</p><p>​在代码中，放在适当的 for 循环内，这个公式转化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><ul><li><strong>计算db</strong></li></ul><p>​这是计算相对于某一特定滤波器 $W_c$ 的成本的 $db$ 的公式：</p><p>$$ db &#x3D; \sum_h \sum_w dZ_{hw} \tag{3}$$</p><p>​正如你在基础神经网络中之前所见，$db$ 是通过求和 $dZ$ 来计算的。在这种情况下，你只是对卷积输出 (Z) 相对于成本的所有梯度求和。</p><p>​在代码中，放在适当的 for 循环内，这个公式转化为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p><strong>代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv_backward</span>(<span class="params">dZ, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现卷积函数的反向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    dZ -- 相对于卷积层输出 (Z) 的成本梯度，形状为 (m, n_H, n_W, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    cache -- conv_backward() 所需的值的缓存，conv_forward() 的输出</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    dA_prev -- 相对于卷积层输入 (A_prev) 的成本梯度，</span></span><br><span class="line"><span class="string">               形状为 (m, n_H_prev, n_W_prev, n_C_prev) 的 numpy 数组</span></span><br><span class="line"><span class="string">    dW -- 相对于卷积层权重 (W) 的成本梯度，</span></span><br><span class="line"><span class="string">          形状为 (f, f, n_C_prev, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    db -- 相对于卷积层偏置 (b) 的成本梯度，</span></span><br><span class="line"><span class="string">          形状为 (1, 1, 1, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ###</span></span><br><span class="line">    <span class="comment"># 从 &quot;cache&quot; 中检索信息</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 A_prev 的形状中检索维度</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 W 的形状中检索维度</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 &quot;hparameters&quot; 中检索信息</span></span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    pad = hparameters[<span class="string">&quot;pad&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 dZ 的形状中检索维度</span></span><br><span class="line">    (m, n_H, n_W, n_C) = dZ.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用正确的形状初始化 dA_prev, dW, db</span></span><br><span class="line">    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    dW = np.zeros((f, f, n_C_prev, n_C))</span><br><span class="line">    db = np.zeros((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对 A_prev 和 dA_prev 进行填充</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                       <span class="comment"># 遍历训练样本</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 从 A_prev_pad 和 dA_prev_pad 中选择第 i 个训练样本</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i, :, :, :]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i, :, :, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                   <span class="comment"># 在输出体积的垂直轴上遍历</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):               <span class="comment"># 在输出体积的水平轴上遍历</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):           <span class="comment"># 遍历输出体积的通道</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到当前“切片”的角落</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 使用角落来定义 a_prev_pad 的切片</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start : vert_end, horiz_start : horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 使用上面给出的公式更新窗口和过滤器参数的梯度</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># 将第 i 个训练样本的 dA_prev 设置为未填充的 da_prev_pad（提示：使用 X[pad:-pad, pad:-pad, :]）</span></span><br><span class="line">        dA_prev[i, :, :, :] = da_prev_pad[pad : -pad, pad : -pad, :]</span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保输出形状是正确的</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">dA, dW, db = conv_backward(Z, cache_conv)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dA_mean =&quot;</span>, np.mean(dA))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dW_mean =&quot;</span>, np.mean(dW))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;db_mean =&quot;</span>, np.mean(db))</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dA_mean = <span class="number">1.4524377775388075</span></span><br><span class="line">dW_mean = <span class="number">1.7269914583139097</span></span><br><span class="line">db_mean = <span class="number">7.839232564616838</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5Pooling-forward（池化前向传播）"><a href="#2-2-5Pooling-forward（池化前向传播）" class="headerlink" title="2.2.5Pooling forward（池化前向传播）"></a>2.2.5Pooling forward（池化前向传播）</h4><p><img src="https://s2.loli.net/2023/12/04/bgrqfALcXpejuWF.png" alt="image-20231204102558855"></p><p>​由于没有填充，将池化的输出形状与输入形状绑定的公式是：</p><p> $$ n_H &#x3D; \lfloor \frac{n_{H_{prev}} - f}{stride} \rfloor +1 $$ </p><p>$$ n_W &#x3D; \lfloor \frac{n_{W_{prev}} - f}{stride} \rfloor +1 $$</p><p> $$ n_C &#x3D; n_{C_{prev}}$$</p><p><strong>代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评分函数：pool_forward</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool_forward</span>(<span class="params">A_prev, hparameters, mode = <span class="string">&quot;max&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现池化层的前向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    A_prev -- 输入数据，形状为 (m, n_H_prev, n_W_prev, n_C_prev) 的 numpy 数组</span></span><br><span class="line"><span class="string">    hparameters -- 包含 &quot;f&quot; 和 &quot;stride&quot; 的 python 字典</span></span><br><span class="line"><span class="string">    mode -- 您想要使用的池化模式，定义为一个字符串（&quot;max&quot; 或 &quot;average&quot;）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    A -- 池化层的输出，形状为 (m, n_H, n_W, n_C) 的 numpy 数组</span></span><br><span class="line"><span class="string">    cache -- 在池化层的反向传播中使用的缓存，包含输入和 hparameters</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从输入形状中检索维度</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 &quot;hparameters&quot; 中检索超参数</span></span><br><span class="line">    f = hparameters[<span class="string">&quot;f&quot;</span>]</span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义输出的维度</span></span><br><span class="line">    n_H = <span class="built_in">int</span>(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = <span class="built_in">int</span>(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化输出矩阵 A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                           <span class="comment"># 遍历训练样本</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                     <span class="comment"># 在输出体积的垂直轴上遍历</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):                 <span class="comment"># 在输出体积的水平轴上遍历</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span> (n_C):            <span class="comment"># 遍历输出体积的通道</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到当前“切片”的角落 (≈4 行)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 使用角落来定义 A_prev 的第 i 个训练样本上的当前切片，通道 c。 (≈1 行)</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start : vert_end, horiz_start : horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 在切片上计算池化操作。使用 if 语句区分模式。使用 np.max/np.mean。</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">                        A[i, h, w, c] = np.<span class="built_in">max</span>(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为 pool_backward() 存储输入和 hparameters 在 &quot;cache&quot; 中</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保输出形状是正确的</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;stride&quot;</span> : <span class="number">2</span>, <span class="string">&quot;f&quot;</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mode = max&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A =&quot;</span>, A)</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters, mode = <span class="string">&quot;average&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mode = average&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A =&quot;</span>, A)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mode = <span class="built_in">max</span></span><br><span class="line">A = [[[[<span class="number">1.74481176</span> <span class="number">0.86540763</span> <span class="number">1.13376944</span>]]]</span><br><span class="line"> [[[<span class="number">1.13162939</span> <span class="number">1.51981682</span> <span class="number">2.18557541</span>]]]]</span><br><span class="line"></span><br><span class="line">mode = average</span><br><span class="line">A = [[[[ <span class="number">0.02105773</span> -<span class="number">0.20328806</span> -<span class="number">0.40389855</span>]]]</span><br><span class="line"> [[[-<span class="number">0.22154621</span>  <span class="number">0.51716526</span>  <span class="number">0.48155844</span>]]]]</span><br></pre></td></tr></table></figure><h4 id="2-2-6Pooling-backward（池化反向传播）"><a href="#2-2-6Pooling-backward（池化反向传播）" class="headerlink" title="2.2.6Pooling backward（池化反向传播）"></a>2.2.6Pooling backward（池化反向传播）</h4><p>​在跳入池化层的反向传播之前，你将构建一个名为 <code>create_mask_from_window()</code> 的辅助函数，它执行以下操作：</p><p>$$ X &#x3D; \begin{bmatrix}<br>1 &amp;&amp; 3 \<br>4 &amp;&amp; 2<br>\end{bmatrix} \quad \rightarrow  \quad M &#x3D;\begin{bmatrix}<br>0 &amp;&amp; 0 \<br>1 &amp;&amp; 0<br>\end{bmatrix}\tag{4}$$</p><p>​如你所见，这个函数创建了一个“掩码”矩阵，用来追踪矩阵中的最大值位置。真（1）表示 X 中最大值的位置，其它条目为假（0）。你稍后会看到，平均池化的反向传播过程与此类似，但使用的掩码不同。</p><p><strong>1.辅助函数代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_mask_from_window</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从输入矩阵 x 创建一个掩码，用于标识 x 的最大条目。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    x -- 形状为 (f, f) 的数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mask -- 与窗口形状相同的数组，在与 x 的最大条目相对应的位置包含一个 True。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ### (≈1 行)</span></span><br><span class="line">    mask = (x &gt;= np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">mask = create_mask_from_window(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x = &#x27;</span>, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mask = &quot;</span>, mask)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x =  [[ <span class="number">1.62434536</span> -<span class="number">0.61175641</span> -<span class="number">0.52817175</span>]</span><br><span class="line">  [-<span class="number">1.07296862</span>  <span class="number">0.86540763</span> -<span class="number">2.3015387</span> ]]</span><br><span class="line">mask =  [[ <span class="literal">True</span> <span class="literal">False</span> <span class="literal">False</span>]</span><br><span class="line">     [<span class="literal">False</span> <span class="literal">False</span> <span class="literal">False</span>]]</span><br></pre></td></tr></table></figure><p><strong>2.反向传播 backward pass 算法代码：</strong></p><p>​在最大池化中，对于每个输入窗口，所有对输出的“影响”都来自单个输入值——最大值。在平均池化中，输入窗口的每个元素对输出都有相同的影响。因此，为了实现反向传播，你现在将实现一个反映这一点的辅助函数。</p><p>​例如，如果我们在前向传播中使用 2x2 滤波器进行平均池化，那么你在反向传播中使用的掩码将看起来像：<br>$$ dZ &#x3D; 1 \quad \rightarrow  \quad dZ &#x3D;\begin{bmatrix}<br>1&#x2F;4 &amp;&amp; 1&#x2F;4 \<br>1&#x2F;4 &amp;&amp; 1&#x2F;4<br>\end{bmatrix}\tag{5}$$</p><p>​这意味着 $dZ$ 矩阵中的每个位置都平等地贡献于输出，因为在前向传播中，我们取了平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">distribute_value</span>(<span class="params">dz, shape</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在维度为 shape 的矩阵中分布输入值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    dz -- 输入标量</span></span><br><span class="line"><span class="string">    shape -- 我们想要分布 dz 值的输出矩阵的形状 (n_H, n_W)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    a -- 大小为 (n_H, n_W) 的数组，我们在其中分布了 dz 的值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ###</span></span><br><span class="line">    <span class="comment"># 从 shape 中检索维度 (≈1 行)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算在矩阵上分布的值 (≈1 行)</span></span><br><span class="line">    average = dz / (n_H * n_W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建一个每个条目都是 &quot;average&quot; 值的矩阵 (≈1 行)</span></span><br><span class="line">    a = average * np.ones((n_H, n_W))</span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = distribute_value(<span class="number">2</span>, (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;distributed value =&#x27;</span>, a)</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distributed value = [[<span class="number">0.5</span> <span class="number">0.5</span>]</span><br><span class="line"> [<span class="number">0.5</span> <span class="number">0.5</span>]]</span><br></pre></td></tr></table></figure><ul><li><strong>3.反向传播 Pooling backward 算法代码:</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pool_backward</span>(<span class="params">dA, cache, mode = <span class="string">&quot;max&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现池化层的反向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    dA -- 相对于池化层输出的成本梯度，与 A 的形状相同</span></span><br><span class="line"><span class="string">    cache -- 来自池化层前向传播的缓存输出，包含层的输入和 hparameters</span></span><br><span class="line"><span class="string">    mode -- 您想要使用的池化模式，定义为字符串（&quot;max&quot; 或 &quot;average&quot;）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    dA_prev -- 相对于池化层输入的成本梯度，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### 开始编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从缓存中检索信息 (≈1 行)</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 &quot;hparameters&quot; 中检索超参数 (≈2 行)</span></span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    f = hparameters[<span class="string">&quot;f&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 A_prev 的形状和 dA 的形状中检索维度 (≈2 行)</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape</span><br><span class="line">    m, n_H, n_W, n_C = dA.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用零初始化 dA_prev (≈1 行)</span></span><br><span class="line">    dA_prev = np.zeros((A_prev.shape))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                       <span class="comment"># 遍历训练样本</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 从 A_prev 中选择训练样本 (≈1 行)</span></span><br><span class="line">        a_prev = A_prev[i, :, :, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                   <span class="comment"># 在垂直轴上遍历</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):               <span class="comment"># 在水平轴上遍历</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):           <span class="comment"># 遍历通道（深度）</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到当前“切片”的角落 (≈4 行)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 在两种模式下计算反向传播。</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 使用角落和 &quot;c&quot; 来定义来自 a_prev 的当前切片 (≈1 行)</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start : vert_end, horiz_start : horiz_end, c]</span><br><span class="line">                        <span class="comment"># 从 a_prev_slice 创建掩码 (≈1 行)</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># 将 dA_prev 设置为 dA_prev + (掩码乘以 dA 的正确条目) (≈1 行)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += dA[i, h, w, c] * mask</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 从 dA 获取值 da (≈1 行)</span></span><br><span class="line">                        da = dA[i, h, w, c]</span><br><span class="line">                        <span class="comment"># 定义过滤器的形状为 fxf (≈1 行)</span></span><br><span class="line">                        shape = (f, f)</span><br><span class="line">                        <span class="comment"># 分布它以获得 dA_prev 的正确切片。即添加 da 的分布值。 (≈1 行)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line">    <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保输出形状是正确的</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><p><strong>部署：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;stride&quot;</span> : <span class="number">1</span>, <span class="string">&quot;f&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">dA = np.random.randn(<span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">&quot;max&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mode = max&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mean of dA = &#x27;</span>, np.mean(dA))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;dA_prev[1,1] = &#x27;</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>])  </span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">&quot;average&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mode = average&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;mean of dA = &#x27;</span>, np.mean(dA))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;dA_prev[1,1] = &#x27;</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>]) </span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mode = <span class="built_in">max</span></span><br><span class="line">mean of dA =  <span class="number">0.14571390272918056</span></span><br><span class="line">dA_prev[<span class="number">1</span>,<span class="number">1</span>] =  [[ <span class="number">0.</span>          <span class="number">0.</span>        ]</span><br><span class="line">  [ <span class="number">5.05844394</span> -<span class="number">1.68282702</span>]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.</span>        ]]</span><br><span class="line"></span><br><span class="line">mode = average</span><br><span class="line">mean of dA =  <span class="number">0.14571390272918056</span></span><br><span class="line">dA_prev[<span class="number">1</span>,<span class="number">1</span>] =  [[ <span class="number">0.08485462</span>  <span class="number">0.2787552</span> ]</span><br><span class="line">  [ <span class="number">1.26461098</span> -<span class="number">0.25749373</span>]</span><br><span class="line"> [ <span class="number">1.17975636</span> -<span class="number">0.53624893</span>]]</span><br></pre></td></tr></table></figure><h2 id="3-Convolutional-Neural-Networks-Application"><a href="#3-Convolutional-Neural-Networks-Application" class="headerlink" title="3.Convolutional Neural Networks: Application"></a>3.Convolutional Neural Networks: Application</h2><p>​<a href="https://github.com/dennybritz/cnn-text-classification-tf">https://github.com/dennybritz/cnn-text-classification-tf</a></p>]]></content>
      
      
      <categories>
          
          <category> 传统算法学习 </category>
          
          <category> 吴恩达课程学习 </category>
          
          <category> Convolutional Neural Networks </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2023-12-2：Hyperparameter tuning and Tensorflow using note</title>
      <link href="/2023/12/02/2023-12-2%EF%BC%9AHyperparameter-tuning-and-Tensorflow-using-note/"/>
      <url>/2023/12/02/2023-12-2%EF%BC%9AHyperparameter-tuning-and-Tensorflow-using-note/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1.任务"></a>1.任务</h2><ol><li><font color=green>学习超参数调试技巧并记录</font></li><li><font color=red>利用Jupyter Notebook进行Tensorflow的基本操作进行熟悉(X)</font></li></ol><span id="more"></span><h2 id="2-超参数调试技巧"><a href="#2-超参数调试技巧" class="headerlink" title="2.超参数调试技巧"></a>2.超参数调试技巧</h2><img src="https://s2.loli.net/2023/12/02/B247iQh1UekmES8.png" alt="image-20231202102333826" style="zoom:50%;" /><p>主要对于这几个超参数进行调试：</p><ul><li><strong><font color="red">$\alpha$</font> ：学习率</strong></li><li><strong><font color="yellow">$\beta$</font>  :  momentum（0.9就是一个很好的值）</strong></li><li>**$\beta1,\beta2,\epsilon$**：</li><li><strong>layers</strong>：</li><li><font color="yellow"><strong>hidden units</strong></font>：</li><li><strong>learning rate decay</strong>：</li><li><font color="yellow"><strong>mini-batch size</strong></font>：保证算法运行有效</li></ul><h3 id="2-1随机取值and精确搜索"><a href="#2-1随机取值and精确搜索" class="headerlink" title="2.1随机取值and精确搜索"></a>2.1随机取值and精确搜索</h3><p>​有粗糙到精细的进行参数的搜索，大概思想如下：</p><img src="https://s2.loli.net/2023/12/02/Qf7gDvkyzetn4WC.png" alt="image-20231202101340743" style="zoom:50%;" /><p>​先对这一个范围内的数进行随机均匀取值（合适的步进值）来进行测试，找到效果最好的那个点。</p><img src="https://s2.loli.net/2023/12/02/yqDSKgCILNZeAYR.png" alt="image-20231202101437718" style="zoom: 33%;" /><p>​接着聚集于表现效果最好的参数点，对于其周围进行密集取值找到表现最好的点。</p><p><img src="https://s2.loli.net/2023/12/02/GsFZ4nHKO8kacX1.png" alt="image-20231202101948206"></p><p>​这里提到的均匀取值，可以考虑在对数坐标轴上分段进行随机均匀取值（减少计算量和资源占用）</p><h3 id="2-2Batch-Normalization"><a href="#2-2Batch-Normalization" class="headerlink" title="2.2Batch-Normalization"></a>2.2Batch-Normalization</h3><p><img src="https://s2.loli.net/2023/12/02/DebuSpgtCj12N9O.png" alt="image-20231202104444483"></p><p>​Batch-Normalization是发生在计算z和a之间。</p><img src="https://s2.loli.net/2023/12/02/ZQFw5xj6oRuiKec.png" alt="image-20231202111518984" style="zoom:50%;" /><p>​Tensorflow中只需要一行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.batch-normalization()</span><br></pre></td></tr></table></figure><h3 id="2-3Softmax回归"><a href="#2-3Softmax回归" class="headerlink" title="2.3Softmax回归"></a>2.3Softmax回归</h3><p>​<strong>softmax 回归</strong>(softmax regression)其实是 logistic 回归的一般形式，logistic 回归用于二分类，而 softmax 回归用于<strong>多分类</strong></p><p>​对于输入数据{($x_1,y_1$),($x_2,y_2$),…,($x_m,y_m$)}有$k$个类别，即$y_i∈{1,2,…,k}$，那么 softmax 回归主要估算输入数据$x_i$ 归属于每一类的概率，即:</p><img src="https://s2.loli.net/2023/12/02/HdeR5acf9LUWVBF.png" alt="image-20231202224233242" style="zoom:80%;" /><p>​上面的式子可以用下图形象化的解析:</p><img src="https://s2.loli.net/2023/12/02/mhvk59LQODsdF3f.webp" alt="img" style="zoom:95%;" /><p>​Softmax回归代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset</span>(<span class="params">file_path</span>):</span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = <span class="built_in">open</span>(file_path)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split()</span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, <span class="built_in">float</span>(lineArr[<span class="number">0</span>]), <span class="built_in">float</span>(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(<span class="built_in">int</span>(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data_arr, label_arr, n_class, iters = <span class="number">1000</span>, alpha = <span class="number">0.1</span>, lam = <span class="number">0.01</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    @description: softmax 训练函数</span></span><br><span class="line"><span class="string">    @param &#123;type&#125; </span></span><br><span class="line"><span class="string">    @return: theta 参数</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>    </span><br><span class="line">    n_samples, n_features = data_arr.shape</span><br><span class="line">    n_classes = n_class</span><br><span class="line">    <span class="comment"># 随机初始化权重矩阵</span></span><br><span class="line">    weights = np.random.rand(n_class, n_features)</span><br><span class="line">    <span class="comment"># 定义损失结果</span></span><br><span class="line">    all_loss = <span class="built_in">list</span>()</span><br><span class="line">    <span class="comment"># 计算 one-hot 矩阵</span></span><br><span class="line">    y_one_hot = one_hot(label_arr, n_samples, n_classes)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        <span class="comment"># 计算 m * k 的分数矩阵</span></span><br><span class="line">        scores = np.dot(data_arr, weights.T)</span><br><span class="line">        <span class="comment"># 计算 softmax 的值</span></span><br><span class="line">        probs = softmax(scores)</span><br><span class="line">        <span class="comment"># 计算损失函数值</span></span><br><span class="line">        loss = - (<span class="number">1.0</span> / n_samples) * np.<span class="built_in">sum</span>(y_one_hot * np.log(probs))</span><br><span class="line">        all_loss.append(loss)</span><br><span class="line">        <span class="comment"># 求解梯度</span></span><br><span class="line">        dw = -(<span class="number">1.0</span> / n_samples) * np.dot((y_one_hot - probs).T, data_arr) + lam * weights</span><br><span class="line">        dw[:,<span class="number">0</span>] = dw[:,<span class="number">0</span>] - lam * weights[:,<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 更新权重矩阵</span></span><br><span class="line">        weights  = weights - alpha * dw</span><br><span class="line">    <span class="keyword">return</span> weights, all_loss</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">scores</span>):</span><br><span class="line">    <span class="comment"># 计算总和</span></span><br><span class="line">    sum_exp = np.<span class="built_in">sum</span>(np.exp(scores), axis = <span class="number">1</span>,keepdims = <span class="literal">True</span>)</span><br><span class="line">    softmax = np.exp(scores) / sum_exp</span><br><span class="line">    <span class="keyword">return</span> softmax</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot</span>(<span class="params">label_arr, n_samples, n_classes</span>):</span><br><span class="line">    one_hot = np.zeros((n_samples, n_classes))</span><br><span class="line">    one_hot[np.arange(n_samples), label_arr.T] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">test_dataset, label_arr, weights</span>):</span><br><span class="line">    scores = np.dot(test_dataset, weights.T)</span><br><span class="line">    probs = softmax(scores)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(probs, axis=<span class="number">1</span>).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment">#gen_dataset()</span></span><br><span class="line">    data_arr, label_arr = load_dataset(<span class="string">&#x27;train_dataset.txt&#x27;</span>)</span><br><span class="line">    data_arr = np.array(data_arr)</span><br><span class="line">    label_arr = np.array(label_arr).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    weights, all_loss = train(data_arr, label_arr, n_class = <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算预测的准确率</span></span><br><span class="line">    test_data_arr, test_label_arr = load_dataset(<span class="string">&#x27;test_dataset.txt&#x27;</span>)</span><br><span class="line">    test_data_arr = np.array(test_data_arr)</span><br><span class="line">    test_label_arr = np.array(test_label_arr).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    n_test_samples = test_data_arr.shape[<span class="number">0</span>]</span><br><span class="line">    y_predict = predict(test_data_arr, test_label_arr, weights)</span><br><span class="line">    accuray = np.<span class="built_in">sum</span>(y_predict == test_label_arr) / n_test_samples</span><br><span class="line">    <span class="built_in">print</span>(accuray)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制损失函数</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">    plt.plot(np.arange(<span class="number">1000</span>), all_loss)</span><br><span class="line">    plt.title(<span class="string">&quot;Development of loss during training&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Number of iterations&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><ul><li><strong>准确率：</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9952</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/12/02/3YzFhg9KX8IiOGe.webp" alt="img"></p><p>详细数据集和代码见此处：<a href="https://github.com/HuStanding/ml/tree/master/softmax">https://github.com/HuStanding/ml/tree/master/softmax</a> </p>]]></content>
      
      
      <categories>
          
          <category> 传统算法学习 </category>
          
          <category> 吴恩达课程学习 </category>
          
          <category> Improving Deep Neural Networks Hyperparameter tuning, Regularization and Optimization </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>2023-12-01 Optimization methods</title>
      <link href="/2023/12/01/2023-12-01-Optimization-methods/"/>
      <url>/2023/12/01/2023-12-01-Optimization-methods/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1.任务"></a>1.任务</h2><p>​学习<strong>Gradient Descent</strong>、<strong>Mini-Batch Gradient descent</strong>、<strong>Momentum</strong>、<strong>Adam（RMSprop和Momentum的结合）</strong></p><span id="more"></span><h2 id="2-准备工作"><a href="#2-准备工作" class="headerlink" title="2.准备工作"></a>2.准备工作</h2><ul><li><strong>导入依赖包</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h2 id="3-四种算法介绍"><a href="#3-四种算法介绍" class="headerlink" title="3.四种算法介绍"></a>3.四种算法介绍</h2><h3 id="3-1Gradient-Descent"><a href="#3-1Gradient-Descent" class="headerlink" title="3.1Gradient Descent"></a>3.1Gradient Descent</h3><p>​Gradient Descent是机器学习中一个简单的优化算法，简称GD。当对每个步骤上的所有 $m$ 示例采取梯度步骤时，它也称为批量梯度下降。</p><ul><li><strong>热身训练</strong></li></ul><p>​实现梯度下降更新规则。梯度下降规则是，对于 $l &#x3D; 1, …, L$: 来说</p><p>​$$ W^{[l]} &#x3D; W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$</p><p>​$$ b^{[l]} &#x3D; b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$</p><p>​在这段话中，“L”表示层数，“$\alpha$”表示学习率。所有参数应该存储在<code>parameters</code>字典中。注意迭代器<code>l</code>在<code>for</code>循环中从0开始，而第一个参数是$W^{[1]}$和$b^{[1]}$。编码时需要将<code>l</code>转换为<code>l+1</code>。</p><p>​下面我们实现这个算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 使用梯度下降更新参数的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters_with_gd</span>(<span class="params">parameters, grads, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用梯度下降的一步来更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    parameters -- 包含要更新的参数的python字典：</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- 包含用于更新每个参数的梯度的python字典：</span></span><br><span class="line"><span class="string">                    grads[&#x27;dW&#x27; + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads[&#x27;db&#x27; + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率，标量。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后参数的python字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络中的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个参数的更新规则</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        <span class="comment">### 开始编写代码 ### (大约 2 行)</span></span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### 结束编写代码 ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><ul><li>部署这段代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取测试数据</span></span><br><span class="line">parameters, grads, learning_rate = update_parameters_with_gd_test_case()</span><br><span class="line"><span class="comment">#运行算法</span></span><br><span class="line">parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br></pre></td></tr></table></figure><ul><li>输出</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ 1.63535156, -0.62320365, -0.53718766],</span><br><span class="line">      [-1.07799357,  0.85639907, -2.29470142]]</span><br><span class="line"></span><br><span class="line">b1 = [[ 1.74604067],</span><br><span class="line">      [-0.75184921]]</span><br><span class="line"></span><br><span class="line">W2 = [[ 0.32171798, -0.25467393,  1.46902454],</span><br><span class="line">      [-2.05617317, -0.31554548, -0.3756023 ],</span><br><span class="line">      [ 1.1404819,  -1.09976462, -0.1612551 ]]</span><br><span class="line"></span><br><span class="line">b2 = [[-0.88020257],</span><br><span class="line">      [ 0.02561572],</span><br><span class="line">      [ 0.57539477]]</span><br></pre></td></tr></table></figure><p>​这里有一种变体叫做随机梯度下降（SGD），它等同于小批量梯度下降，但每个小批量仅包含一个样本。我刚刚实现的更新规则并不会改变。改变的是，我将一次只对一个训练样本计算梯度，而不是对整个训练集计算。下面的代码示例展示了随机梯度下降和（批量）梯度下降之间的区别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###(Batch) Gradient Descent:###</span></span><br><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line"><span class="comment">###Stochastic Gradient Descent:###</span></span><br><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><p>​在随机梯度下降中，更新梯度之前只使用一个训练样本。当训练集很大时，随机梯度下降可以更快。但是，参数会向最小值“振荡”，而不是平滑地收敛。这里有一个这种情况的示意图：</p><p><img src="https://s2.loli.net/2023/12/01/nGQlvMCy53rijzu.png" alt="image-20231201100551931"></p><p>​在实际应用中，如果既不使用整个训练集，也不仅使用一个训练样本来进行每次更新，通常会得到更快的结果。Mini-Batch Gradient descent使用中间数量的样本来进行每一步的更新。使用Mini-Batch Gradient descent时，训练时会遍历小批量样本而不是单个训练样本。</p><p><img src="https://s2.loli.net/2023/12/01/6cEs3wQZIAntJCW.png" alt="image-20231201100521508"></p><p>​我们应该额外关注的是：</p><ul><li>梯度下降、小批量梯度下降和随机梯度下降之间的区别在于你用来进行一次更新步骤的样本数量。</li><li>你需要调整学习率超参数$\alpha$。</li><li>使用一个调整得当的小批量大小，通常它会优于梯度下降或随机梯度下降（特别是当训练集很大时）</li></ul><h3 id="3-2Mini-Batch-Gradient-descent"><a href="#3-2Mini-Batch-Gradient-descent" class="headerlink" title="3.2Mini-Batch Gradient descent"></a>3.2Mini-Batch Gradient descent</h3><p>​该算法主要分为两步：</p><ul><li><strong>Shuffle（洗牌）</strong></li></ul><p>​如下所示，创建一个训练集（X, Y）的打乱版本。X和Y的每一列代表一个训练样本。注意，X和Y之间的随机洗牌是同步进行的。这样在洗牌后，X的第$i^{th}$列是与Y中第$i^{th}$标签对应的样本。洗牌步骤确保样本将随机分配到不同的小批量中。</p><p><img src="https://s2.loli.net/2023/12/01/KgDklCsHr3RBNqU.png" alt="kiank_shuffle"></p><ul><li><strong>Partition（分割）</strong></li></ul><p>​将打乱的（X, Y）分割成大小为<code>mini_batch_size</code>（这里是64）的小批量。注意，训练样本的数量并不总是能被<code>mini_batch_size</code>整除。最后一个小批量可能会更小，但这不需要担心。当最终的小批量小于完整的<code>mini_batch_size</code>时，它将如下所示：</p><p><img src="https://s2.loli.net/2023/12/01/2e7FkKp4DrHCo1j.png" alt="kiank_partition"></p><p>​下面我们实现这个算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 随机生成小批量的函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">random_mini_batches</span>(<span class="params">X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从 (X, Y) 中创建随机小批量的列表</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 输入数据，形状为 (输入大小, 样本数量)</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量（蓝点为1 / 红点为0），形状为 (1, 样本数量)</span></span><br><span class="line"><span class="string">    mini_batch_size -- 小批量的大小，整数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    mini_batches -- 同步的 (mini_batch_X, mini_batch_Y) 列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(seed)            <span class="comment"># 为了使你的“随机”小批量与我们的相同</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># 训练样本的数量</span></span><br><span class="line">    mini_batches = []</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 第一步：洗牌 (X, Y)</span></span><br><span class="line">    permutation = <span class="built_in">list</span>(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二步：分割 (shuffled_X, shuffled_Y)，排除最后一个案例。</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># 在你的分割中，大小为 mini_batch_size 的小批量的数量</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + <span class="number">1</span>) * mini_batch_size]</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理最后一个案例（最后一个小批量 &lt; mini_batch_size）</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><ul><li><strong>部署代码：</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()</span><br><span class="line">mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 1st mini_batch_X: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 2nd mini_batch_X: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">1</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 3rd mini_batch_X: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">2</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 1st mini_batch_Y: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">0</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 2nd mini_batch_Y: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">1</span>][<span class="number">1</span>].shape)) </span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;shape of the 3rd mini_batch_Y: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">2</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;mini batch sanity check: &quot;</span> + <span class="built_in">str</span>(mini_batches[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>:<span class="number">3</span>]))</span><br></pre></td></tr></table></figure><ul><li><strong>输出：</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">shape of the 1st mini_batch_X: (12288, 64)</span><br><span class="line">shape of the 2nd mini_batch_X: (12288, 64)</span><br><span class="line">shape of the 3rd mini_batch_X: (12288, 20)</span><br><span class="line">shape of the 1st mini_batch_Y: (1, 64)</span><br><span class="line">shape of the 2nd mini_batch_Y: (1, 64)</span><br><span class="line">shape of the 3rd mini_batch_Y: (1, 20)</span><br><span class="line">mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]</span><br></pre></td></tr></table></figure><h3 id="3-3Momentum"><a href="#3-3Momentum" class="headerlink" title="3.3Momentum"></a>3.3Momentum</h3><p>​由于Mini-Batch Gradient descent在仅看到示例子集后进行参数更新，因此更新的方向存在一定方差，因此Mini-Batch Gradient descent所采取的路径将“振荡”收敛。 利用Momentum可以减少这些振荡。</p><p>​Momentum考虑了过去的梯度来平滑更新。 我们将先前梯度的“方向”存储在变量 $v$ 中。 形式上，这将是先前步骤梯度的指数加权平均值。 您还可以将 $v$ 视为滚下山的球的“速度”，根据山坡的梯度&#x2F;坡度方向增加速度（和动量）。</p><p><img src="https://s2.loli.net/2023/12/01/nvGS3BZxrVbTtyo.png" alt="Momentum"></p><p>​下面我们实现这个算法：</p><p>​首先是初始化速度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 初始化速度</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_velocity</span>(<span class="params">parameters</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    以python字典的形式初始化速度，其中：</span></span><br><span class="line"><span class="string">                - 键： &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span></span><br><span class="line"><span class="string">                - 值：与相应梯度/参数形状相同的零值numpy数组。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    parameters -- 包含你的参数的python字典。</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    v -- 包含当前速度的python字典。</span></span><br><span class="line"><span class="string">                    v[&#x27;dW&#x27; + str(l)] = dWl的速度</span></span><br><span class="line"><span class="string">                    v[&#x27;db&#x27; + str(l)] = dbl的速度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络中的层数</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化速度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        v[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><ul><li><strong>部署代码：</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_velocity_test_case()</span><br><span class="line"></span><br><span class="line">v = initialize_velocity(parameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db2&quot;</span>]))</span><br></pre></td></tr></table></figure><ul><li><strong>输出：</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">v[&quot;dW1&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;db1&quot;] = [[0.0],</span><br><span class="line">            [0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW2&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;db2&quot;] = [[0.0],</span><br><span class="line">            [0.0],</span><br><span class="line">            [0.0]]</span><br></pre></td></tr></table></figure><p>​接着是momentum算法实现参数的更新。动量更新规则是，对于 $l &#x3D; 1, …, L$：</p><p>​$$ \begin{cases}<br>v_{dW^{[l]}} &#x3D; \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>W^{[l]} &#x3D; W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}\tag{3}$$</p><p>​$$\begin{cases}<br>v_{db^{[l]}} &#x3D; \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>b^{[l]} &#x3D; b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}\tag{4}$$</p><p>​其中L是层数，$\beta$ 是动量，$\alpha$ 是学习率。所有参数应该存储在<code>parameters</code>字典中。注意迭代器<code>l</code>在<code>for</code>循环中从0开始，而第一个参数是 $W^{[1]}$ 和 $b^{[1]}$（这里的上标是“一”）。因此，在编码时需要将<code>l</code>改为<code>l+1</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 使用动量更新参数的函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters_with_momentum</span>(<span class="params">parameters, grads, v, beta, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用动量方法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    parameters -- 包含你的参数的python字典：</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- 包含每个参数梯度的python字典：</span></span><br><span class="line"><span class="string">                    grads[&#x27;dW&#x27; + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads[&#x27;db&#x27; + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- 包含当前速度的python字典：</span></span><br><span class="line"><span class="string">                    v[&#x27;dW&#x27; + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v[&#x27;db&#x27; + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- 动量超参数，标量</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率，标量</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后参数的python字典 </span></span><br><span class="line"><span class="string">    v -- 包含更新后速度的python字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络中的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每个参数的动量更新</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 4 行)</span></span><br><span class="line">        <span class="comment"># 计算速度</span></span><br><span class="line">        v[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta * v[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta * v[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * v[<span class="string">&#x27;dW&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * v[<span class="string">&#x27;db&#x27;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><ul><li><strong>部署算法</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v = update_parameters_with_momentum_test_case()</span><br><span class="line"></span><br><span class="line">parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = <span class="number">0.9</span>, learning_rate = <span class="number">0.01</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db2&quot;</span>]))</span><br></pre></td></tr></table></figure><ul><li><strong>输出</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ 1.62544598, -0.61290114, -0.52907334],</span><br><span class="line">      [-1.07347112,  0.86450677, -2.30085497]]</span><br><span class="line"></span><br><span class="line">b1 = [[ 1.74493465],</span><br><span class="line">      [-0.76027113]]</span><br><span class="line"></span><br><span class="line">W2 = [[ 0.31930698, -0.24990073,  1.4627996 ],</span><br><span class="line">      [-2.05974396, -0.32173003, -0.38320915],</span><br><span class="line">      [ 1.13444069, -1.0998786,  -0.1713109 ]]</span><br><span class="line"></span><br><span class="line">b2 = [[-0.87809283],</span><br><span class="line">      [ 0.04055394],</span><br><span class="line">      [ 0.58207317]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW1&quot;] = [[-0.11006192,  0.11447237,  0.09015907],</span><br><span class="line">            [ 0.05024943,  0.09008559, -0.06837279]]</span><br><span class="line"></span><br><span class="line">v[&quot;db1&quot;] = [[-0.01228902],</span><br><span class="line">            [-0.09357694]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW2&quot;] = [[-0.02678881,  0.05303555, -0.06916608],</span><br><span class="line">            [-0.03967535, -0.06871727, -0.08452056],</span><br><span class="line">            [-0.06712461, -0.00126646, -0.11173103]]</span><br><span class="line"></span><br><span class="line">v[&quot;db2&quot;] = [[ 0.02344157],</span><br><span class="line">            [ 0.16598022],</span><br><span class="line">            [ 0.07420442]]</span><br></pre></td></tr></table></figure><p>​<font color="red"><strong>这个算法需要额外注意的是</strong>：</font> </p><ul><li>速度是以<strong>0</strong>开始初始化的。因此，算法需要几次迭代来“积累”速度并开始采取更大的步伐。</li><li>如果 $\beta &#x3D; 0$，那么这就变成了没有动量的标准梯度下降。</li></ul><p>​<font color="red"><strong>如何选择$\beta$？</strong></font> </p><ul><li>动量 $\beta$ 越大，更新就越平滑，因为我们考虑了更多过去的梯度。但是，如果 $\beta$ 太大，也可能过度平滑更新。</li><li>$\beta$ 的常见值范围从 0.8 到 0.999。如果你不想调整这个值，$\beta &#x3D; 0.9$ 通常是一个合理的默认选择。</li><li>为你的模型调整最优的 $\beta$ 可能需要尝试多个值，以查看哪个在降低成本函数 $J$ 的值方面效果最好。</li></ul><h3 id="3-4Adam"><a href="#3-4Adam" class="headerlink" title="3.4Adam"></a>3.4Adam</h3><ul><li><strong>工作机制</strong></li></ul><ol><li>它计算过去梯度的指数加权平均值，并将其存储在变量 $v$（未进行偏差校正前）和 $v^{corrected}$（进行偏差校正后）中。</li><li>它计算过去梯度平方的指数加权平均值，并将其存储在变量 $s$（未进行偏差校正前）和 $s^{corrected}$（进行偏差校正后）中。</li><li>它基于结合“1”和“2”的信息来更新参数。</li></ol><p>​更新规则，对于 $l &#x3D; 1, …, L$：</p><p><img src="https://s2.loli.net/2023/12/01/WH7sBKt2PDXEjnZ.png" alt="image-20231201113047855"></p><p>其中： </p><ul><li>t 表示 Adam 执行的步数 </li><li>L 是层数 </li><li>$\beta_1$ 和 $\beta_2$ 是控制两个指数加权平均的超参数。 </li><li>$\alpha$ 是学习率 </li><li>$\varepsilon$ 是一个非常小的数，用来避免除以零 像往常一样，我们会将所有参数存储在 <code>parameters</code> 字典中。</li></ul><p>​下面我们实现这个算法：</p><p>​首先是初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 初始化Adam算法的函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_adam</span>(<span class="params">parameters</span>) :</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    以两个python字典的形式初始化 v 和 s：</span></span><br><span class="line"><span class="string">                - 键： &quot;dW1&quot;, &quot;db1&quot;, ..., &quot;dWL&quot;, &quot;dbL&quot; </span></span><br><span class="line"><span class="string">                - 值：形状与相应梯度/参数相同的零值numpy数组。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    parameters -- 包含你的参数的python字典。</span></span><br><span class="line"><span class="string">                    parameters[&quot;W&quot; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&quot;b&quot; + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回： </span></span><br><span class="line"><span class="string">    v -- 将包含梯度指数加权平均的python字典。</span></span><br><span class="line"><span class="string">                    v[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- 将包含平方梯度指数加权平均的python字典。</span></span><br><span class="line"><span class="string">                    s[&quot;dW&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s[&quot;db&quot; + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span> <span class="comment"># 神经网络中的层数</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化 v, s。输入：&quot;parameters&quot;。输出：&quot;v, s&quot;。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">    <span class="comment">### 开始编码 ### (大约 4 行)</span></span><br><span class="line">        v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)].shape)</span><br><span class="line">    <span class="comment">### 结束编码 ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><ul><li><strong>部署代码</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_adam_test_case()</span><br><span class="line"></span><br><span class="line">v, s = initialize_adam(parameters)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;db2&quot;</span>]))</span><br></pre></td></tr></table></figure><ul><li><strong>输出</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">v[&quot;dW1&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;db1&quot;] = [[0.0],</span><br><span class="line">            [0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW2&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">v[&quot;db2&quot;] = [[0.0],</span><br><span class="line">            [0.0],</span><br><span class="line">            [0.0]]</span><br><span class="line"></span><br><span class="line">s[&quot;dW1&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">s[&quot;db1&quot;] = [[0.0],</span><br><span class="line">            [0.0]]</span><br><span class="line"></span><br><span class="line">s[&quot;dW2&quot;] = [[0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0],</span><br><span class="line">            [0.0, 0.0, 0.0]]</span><br><span class="line"></span><br><span class="line">s[&quot;db2&quot;] = [[0.0],</span><br><span class="line">            [0.0],</span><br><span class="line">            [0.0]]</span><br></pre></td></tr></table></figure><p>​接着我们利用写好的初始化函数和上述提到的Adam算法公式来写出对应的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 使用Adam更新参数的函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters_with_adam</span>(<span class="params">parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,</span></span><br><span class="line"><span class="params">                                beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用Adam方法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    parameters -- 包含你的参数的python字典：</span></span><br><span class="line"><span class="string">                    parameters[&#x27;W&#x27; + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters[&#x27;b&#x27; + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- 包含每个参数梯度的python字典：</span></span><br><span class="line"><span class="string">                    grads[&#x27;dW&#x27; + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads[&#x27;db&#x27; + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam变量，第一梯度的移动平均值，python字典</span></span><br><span class="line"><span class="string">    s -- Adam变量，平方梯度的移动平均值，python字典</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率，标量。</span></span><br><span class="line"><span class="string">    beta1 -- 第一时刻估计的指数衰减超参数</span></span><br><span class="line"><span class="string">    beta2 -- 第二时刻估计的指数衰减超参数</span></span><br><span class="line"><span class="string">    epsilon -- 防止Adam更新中除以零的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后参数的python字典 </span></span><br><span class="line"><span class="string">    v -- Adam变量，第一梯度的移动平均值，python字典</span></span><br><span class="line"><span class="string">    s -- Adam变量，平方梯度的移动平均值，python字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>                 <span class="comment"># 神经网络中的层数</span></span><br><span class="line">    v_corrected = &#123;&#125;                         <span class="comment"># 初始化第一时刻估计，python字典</span></span><br><span class="line">    s_corrected = &#123;&#125;                         <span class="comment"># 初始化第二时刻估计，python字典</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对所有参数执行Adam更新</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">        <span class="comment"># 计算梯度的移动平均值。输入：&quot;v, grads, beta1&quot;。输出：&quot;v&quot;。</span></span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta1 * v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta1 * v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算校正后的第一时刻估计。输入：&quot;v, beta1, t&quot;。输出：&quot;v_corrected&quot;。</span></span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        v_corrected[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = v[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">        v_corrected[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = v[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算平方梯度的移动平均值。输入：&quot;s, grads, beta2&quot;。输出：&quot;s&quot;。</span></span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        s[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta2 * s[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.multiply(grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)])</span><br><span class="line">        s[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = beta2 * s[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.multiply(grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)])</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算校正后的第二原始时刻估计。输入：&quot;s, beta2, t&quot;。输出：&quot;s_corrected&quot;。</span></span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        s_corrected[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = s[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">        s_corrected[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = s[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数。输入：&quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;。输出：&quot;parameters&quot;。</span></span><br><span class="line">        <span class="comment">### 开始编码 ### (大约 2 行)</span></span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * v_corrected[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (epsilon + np.sqrt(s_corrected[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]))</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] -= learning_rate * v_corrected[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] / (epsilon + np.sqrt(s_corrected[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)]))</span><br><span class="line">        <span class="comment">### 结束编码 ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure><ul><li><strong>代码部署</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v, s = update_parameters_with_adam_test_case()</span><br><span class="line">parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;b2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;b2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;v[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(v[<span class="string">&quot;db2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;dW1\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;dW1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;db1\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;db1&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;dW2\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;dW2&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;s[\&quot;db2\&quot;] = &quot;</span> + <span class="built_in">str</span>(s[<span class="string">&quot;db2&quot;</span>]))</span><br></pre></td></tr></table></figure><ul><li><strong>输出</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">W1 = [[ 1.63178673, -0.61919778, -0.53561312],</span><br><span class="line">      [-1.08040999,  0.85796626, -2.29409733]]</span><br><span class="line"></span><br><span class="line">b1 = [[ 1.75225313],</span><br><span class="line">      [-0.75376553]]</span><br><span class="line"></span><br><span class="line">W2 = [[ 0.32648046, -0.25681174,  1.46954931],</span><br><span class="line">      [-2.05269934, -0.31497584, -0.37661299],</span><br><span class="line">      [ 1.14121081, -1.09244991, -0.16498684]]</span><br><span class="line"></span><br><span class="line">b2 = [[-0.88529979],</span><br><span class="line">      [ 0.03477238],</span><br><span class="line">      [ 0.57537385]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW1&quot;] = [[-0.11006192,  0.11447237,  0.09015907],</span><br><span class="line">            [ 0.05024943,  0.09008559, -0.06837279]]</span><br><span class="line"></span><br><span class="line">v[&quot;db1&quot;] = [[-0.01228902],</span><br><span class="line">            [-0.09357694]]</span><br><span class="line"></span><br><span class="line">v[&quot;dW2&quot;] = [[-0.02678881,  0.05303555, -0.06916608],</span><br><span class="line">            [-0.03967535, -0.06871727, -0.08452056],</span><br><span class="line">            [-0.06712461, -0.00126646, -0.11173103]]</span><br><span class="line"></span><br><span class="line">v[&quot;db2&quot;] = [[ 0.02344157],</span><br><span class="line">            [ 0.16598022],</span><br><span class="line">            [ 0.07420442]]</span><br><span class="line"></span><br><span class="line">s[&quot;dW1&quot;] = [[0.00121136, 0.00131039, 0.00081287],</span><br><span class="line">            [0.0002525,  0.00081154, 0.00046748]]</span><br><span class="line"></span><br><span class="line">s[&quot;db1&quot;] = [[1.51020075e-05],</span><br><span class="line">            [8.75664434e-04]]</span><br><span class="line"></span><br><span class="line">s[&quot;dW2&quot;] = [[7.17640232e-05, 2.81276921e-04, 4.78394595e-04],</span><br><span class="line">            ...</span><br><span class="line">            [4.50571368e-04, 1.60392066e-07, 1.24838242e-03]]</span><br><span class="line"></span><br><span class="line">s[&quot;db2&quot;] = [[5.49507194e-05],</span><br><span class="line">            [2.75494327e-03],</span><br><span class="line">            [5.50629536e-04]]</span><br></pre></td></tr></table></figure><h2 id="4四种算法在模型中的可视化对比"><a href="#4四种算法在模型中的可视化对比" class="headerlink" title="4四种算法在模型中的可视化对比"></a>4四种算法在模型中的可视化对比</h2><h3 id="4-1准备工作"><a href="#4-1准备工作" class="headerlink" title="4.1准备工作"></a>4.1准备工作</h3><ul><li><strong>导入数据</strong></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure><ul><li><strong>定义可视化图函数用于观察三种优化函数的效果</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_boundary</span>(<span class="params">model, X, y</span>):</span><br><span class="line">    <span class="comment">#import pdb;pdb.set_trace()</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].<span class="built_in">min</span>() - <span class="number">1</span>, X[<span class="number">0</span>, :].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].<span class="built_in">min</span>() - <span class="number">1</span>, X[<span class="number">1</span>, :].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;x2&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x1&#x27;</span>)</span><br><span class="line">    y = y.reshape(X[<span class="number">0</span>,:].shape)<span class="comment">#must reshape,otherwise confliction with dimensions</span></span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=y, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="4-2定义模型"><a href="#4-2定义模型" class="headerlink" title="4.2定义模型"></a>4.2定义模型</h3><p>我们已经实现了一个三层神经网络。你将用以下方法来训练它：</p><ul><li><p>Mini-batch Gradient Descent：它会调用你的函数：</p><ul><li><code>update_parameters_with_gd()</code></li></ul></li><li><p>Mini-batchMomentum：它会调用你的函数：</p><ul><li><code>initialize_velocity()</code> 和 <code>update_parameters_with_momentum()</code></li></ul></li><li><p>Mini-batch Adam：它会调用你的函数：</p><ul><li><code>initialize_adam()</code> 和 <code>update_parameters_with_adam()</code></li></ul></li><li><p><strong>模型代码</strong>：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">X, Y, layers_dims, optimizer, learning_rate=<span class="number">0.0007</span>, mini_batch_size=<span class="number">64</span>, beta=<span class="number">0.9</span>,</span></span><br><span class="line"><span class="params">          beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>, num_epochs=<span class="number">10000</span>, print_cost=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    可以在不同优化器模式下运行的三层神经网络模型。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    X -- 输入数据，形状为 (2, 样本数量)</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量（蓝点为1 / 红点为0），形状为 (1, 样本数量)</span></span><br><span class="line"><span class="string">    layers_dims -- python列表，包含每层的大小</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率，标量。</span></span><br><span class="line"><span class="string">    mini_batch_size -- 小批量的大小</span></span><br><span class="line"><span class="string">    beta -- 动量超参数</span></span><br><span class="line"><span class="string">    beta1 -- 过去梯度估计的指数衰减超参数</span></span><br><span class="line"><span class="string">    beta2 -- 过去平方梯度估计的指数衰减超参数</span></span><br><span class="line"><span class="string">    epsilon -- 防止Adam更新中除以零的超参数</span></span><br><span class="line"><span class="string">    num_epochs -- 迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- 如果为True，每1000次迭代打印一次成本</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后参数的python字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    L = <span class="built_in">len</span>(layers_dims)             <span class="comment"># 神经网络中的层数</span></span><br><span class="line">    costs = []                       <span class="comment"># 用于跟踪成本</span></span><br><span class="line">    t = <span class="number">0</span>                            <span class="comment"># 初始化Adam更新所需的计数器</span></span><br><span class="line">    seed = <span class="number">10</span>                        <span class="comment"># 出于评分目的，确保你的“随机”小批量与我们的相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化优化器</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">&quot;gd&quot;</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 梯度下降不需要初始化</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">&quot;momentum&quot;</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">&quot;adam&quot;</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义随机小批量。每个周期后我们增加种子以不同地重洗数据集</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 选择一个小批量</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 正向传播</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算成本</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">&quot;gd&quot;</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">&quot;momentum&quot;</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">&quot;adam&quot;</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam计数器</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2, epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每1000次迭代打印成本</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;迭代次数 %i 后的成本: %f&quot;</span> % (i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 绘制成本图</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;成本&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;迭代次数（每100次）&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;学习率 = &quot;</span> + <span class="built_in">str</span>(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="4-3对比算法效果"><a href="#4-3对比算法效果" class="headerlink" title="4.3对比算法效果"></a>4.3对比算法效果</h3><h4 id="4-3-1Mini-batch-Gradient-descent"><a href="#4-3-1Mini-batch-Gradient-descent" class="headerlink" title="4.3.1Mini-batch Gradient descent"></a>4.3.1<strong>Mini-batch Gradient descent</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">&quot;gd&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">&quot;Model with Gradient Descent optimization&quot;</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([-<span class="number">1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([-<span class="number">1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/12/01/7Kk5OZBGLqHxdtP.png" alt="image-20231201120226785"></p><p><strong>Accuracy: 0.7966666666666666</strong></p><p><img src="https://s2.loli.net/2023/12/01/4x7IvSq9btPp1go.png" alt="image-20231201120238291"></p><h4 id="4-3-2Mini-batch-gradient-descent-with-momentum"><a href="#4-3-2Mini-batch-gradient-descent-with-momentum" class="headerlink" title="4.3.2Mini-batch gradient descent with momentum"></a>4.3.2<strong>Mini-batch gradient descent with momentum</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">&quot;momentum&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">&quot;Model with Momentum optimization&quot;</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([-<span class="number">1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([-<span class="number">1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/12/01/EhJ8kBYyq3RtDW7.png" alt="image-20231201120325570"></p><p><strong>Accuracy: 0.7966666666666666</strong></p><p><img src="https://s2.loli.net/2023/12/01/AIg1WLO6PQSl3c2.png" alt="image-20231201120330219"></p><h4 id="4-3-3Mini-batch-with-Adam-mode"><a href="#4-3-3Mini-batch-with-Adam-mode" class="headerlink" title="4.3.3Mini-batch with Adam mode"></a>4.3.3<strong>Mini-batch with Adam mode</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">&quot;adam&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">&quot;Model with Adam optimization&quot;</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([-<span class="number">1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([-<span class="number">1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/12/01/IB3u9wJcjCKLaSq.png" alt="image-20231201120408163"></p><p><strong>Accuracy: 0.94</strong></p><p><img src="https://s2.loli.net/2023/12/01/Znl46ejU8SB7N2Y.png" alt="image-20231201120420375"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h2><p>​Momentum通常有所帮助，但鉴于较小的学习率和简单的数据集，其影响几乎可以忽略不计。此外，你在成本中看到的巨大波动来自于一些小批量对优化算法来说比其他批量更难处理的事实。</p><p>另一方面，Adam明显优于mini-batch gradient descent和Momentum。如果在这个简单的数据集上运行更多的迭代，所有三种方法都会导致非常好的结果。然而，你已经看到Adam收敛得更快。</p><p>Adam的一些优点包括：</p><ul><li>相对较低的内存要求（尽管高于梯度下降和带动量的梯度下降）</li><li>通常即使在很少调整超参数的情况下也能表现良好（$\alpha$ 除外）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 传统算法学习 </category>
          
          <category> 吴恩达课程学习 </category>
          
          <category> Improving Deep Neural Networks Hyperparameter tuning, Regularization and Optimization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化算法; </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-11-30：Deep-Neural-Network-Application</title>
      <link href="/2023/11/30/2023-11-30%EF%BC%9ADeep-Neural-Network-Application/"/>
      <url>/2023/11/30/2023-11-30%EF%BC%9ADeep-Neural-Network-Application/</url>
      
        <content type="html"><![CDATA[<h2 id="1-任务"><a href="#1-任务" class="headerlink" title="1.任务"></a>1.任务</h2><p>创建并且部署一个深度神经网络来进行监督学习</p><span id="more"></span><h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h2><h3 id="2-1导入依赖包"><a href="#2-1导入依赖包" class="headerlink" title="2.1导入依赖包"></a>2.1导入依赖包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="2-2导入数据集"><a href="#2-2导入数据集" class="headerlink" title="2.2导入数据集"></a>2.2导入数据集</h3><p>这里使用的是之前案例中的“Cat vs non-Cat”数据集”data.h5”</p><ul><li><strong>数据集介绍：</strong></li></ul><ol><li>标记为猫(1)或非猫(0)的m_train图像的训练集</li><li>m_test图像标记为猫和非猫的测试集</li><li>每个图像的形状是(num_px, num_px, 3)，其中3是3通道(RGB)。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br></pre></td></tr></table></figure><ul><li><strong>数据形状查看：</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Number of training examples: &quot;</span> + <span class="built_in">str</span>(m_train))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Number of testing examples: &quot;</span> + <span class="built_in">str</span>(m_test))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Each image is of size: (&quot;</span> + <span class="built_in">str</span>(num_px) + <span class="string">&quot;, &quot;</span> + <span class="built_in">str</span>(num_px) + <span class="string">&quot;, 3)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;train_x_orig shape: &quot;</span> + <span class="built_in">str</span>(train_x_orig.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;train_y shape: &quot;</span> + <span class="built_in">str</span>(train_y.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;test_x_orig shape: &quot;</span> + <span class="built_in">str</span>(test_x_orig.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;test_y shape: &quot;</span> + <span class="built_in">str</span>(test_y.shape))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Number of training examples: 209</span><br><span class="line">Number of testing examples: 50</span><br><span class="line">Each image is of size: (64, 64, 3)</span><br><span class="line">train_x_orig shape: (209, 64, 64, 3)</span><br><span class="line">train_y shape: (1, 209)</span><br><span class="line">test_x_orig shape: (50, 64, 64, 3)</span><br><span class="line">test_y shape: (1, 50)</span><br></pre></td></tr></table></figure><ul><li><strong>在把它们输入网络之前应该先归一化并reshape！</strong></li></ul><p><img src="https://s2.loli.net/2023/11/30/sgZbQ6maeDF7BvL.png" alt="图片的向量转化"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T</span><br><span class="line">test_x_faltten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>],-<span class="number">1</span>).T</span><br><span class="line"></span><br><span class="line">train_x = train_x_flatten/<span class="number">255</span></span><br><span class="line">test_x = test_x_faltten/<span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;train_x&#x27;s shape: &quot;</span> + <span class="built_in">str</span>(train_x.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;test_x&#x27;s shape: &quot;</span> + <span class="built_in">str</span>(test_x.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;train_x_orig: &quot;</span> + <span class="built_in">str</span>(train_x_orig.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;test_x_orig: &quot;</span> + <span class="built_in">str</span>(test_x_orig.shape))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_x&#x27;s shape: (12288, 209) #209*64*64</span><br><span class="line">test_x&#x27;s shape: (12288, 50)  #50*64*64</span><br><span class="line">train_x_orig: (209, 64, 64, 3)</span><br><span class="line">test_x_orig: (50, 64, 64, 3)</span><br></pre></td></tr></table></figure><h2 id="3-架构模型"><a href="#3-架构模型" class="headerlink" title="3.架构模型"></a>3.架构模型</h2><p>架构两种模型:</p><ul><li>A 2-layer neural network</li><li>An L-layer deep neural network</li></ul><h3 id="3-1-2-layer-neural-network"><a href="#3-1-2-layer-neural-network" class="headerlink" title="3.1  2-layer neural network"></a>3.1  2-layer neural network</h3><img src="https://s2.loli.net/2023/11/30/tMdZKI9ahzUsuXR.png" alt="image-20231130102115721" style="zoom:67%;" /><p>这个模型可以总结为：</p><p><strong>INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT</strong></p><p>具体细节介绍：</p><p><img src="https://s2.loli.net/2023/11/30/knBAQrbEfeP2Lzx.png" alt="image-20231130102249622"></p><h3 id="3-2-L-layer-deep-neural-network"><a href="#3-2-L-layer-deep-neural-network" class="headerlink" title="3.2 L-layer deep neural network"></a>3.2 L-layer deep neural network</h3><img src="https://s2.loli.net/2023/11/30/z1FeoL5gum6pjMK.png" alt="image-20231130102323498" style="zoom:67%;" /><p>这个模型可以归纳为：</p><p><strong>[LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID</strong></p><p>具体细节介绍：</p><p><img src="https://s2.loli.net/2023/11/30/K5kzxdDyuI9fFTH.png" alt="image-20231130102504229"></p><h2 id="4-Two-layer-neural-network实现"><a href="#4-Two-layer-neural-network实现" class="headerlink" title="4. Two-layer neural network实现"></a>4. Two-layer neural network实现</h2><h3 id="4-1定义基本功能"><a href="#4-1定义基本功能" class="headerlink" title="4.1定义基本功能"></a>4.1<strong>定义基本功能</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters</span>(<span class="params">n_x,n_h,n_y</span>):</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#验证矩阵维度是否正确</span></span><br><span class="line">    <span class="keyword">assert</span>((n_h, n_x) == W1.shape)</span><br><span class="line">    <span class="keyword">assert</span>((n_h, <span class="number">1</span>) == b1.shape)</span><br><span class="line">    <span class="keyword">assert</span>((n_y, n_h) == W2.shape)</span><br><span class="line">    <span class="keyword">assert</span>((n_y, <span class="number">1</span>) == b2.shape)</span><br><span class="line">    <span class="comment">#整合参数到parameters中输出</span></span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">        <span class="string">&quot;b1&quot;</span>: b1,</span><br><span class="line">        <span class="string">&quot;W2&quot;</span>: W2,</span><br><span class="line">        <span class="string">&quot;b2&quot;</span>: b2</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_forward</span>(<span class="params">A_prev, W, b, activation</span>):</span><br><span class="line">   <span class="comment"># 执行线性变换（矩阵乘法 + 偏置项）</span></span><br><span class="line">   <span class="keyword">if</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">       <span class="comment"># 计算 Z 和线性缓存</span></span><br><span class="line">       Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">       <span class="comment"># 应用 Sigmoid 激活函数并计算激活缓存</span></span><br><span class="line">       A, activation_cache = sigmoid(Z)</span><br><span class="line">   <span class="keyword">elif</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">       <span class="comment"># 计算 Z 和线性缓存</span></span><br><span class="line">       Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">       <span class="comment"># 应用 ReLU 激活函数并计算激活缓存</span></span><br><span class="line">       A, activation_cache = relu(Z)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 检查输出形状是否正确</span></span><br><span class="line">   <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 将线性缓存和激活缓存组合在一起</span></span><br><span class="line">   cache = (linear_cache, activation_cache)</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># 返回输出和缓存</span></span><br><span class="line">   <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算损失值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">AL,Y</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(<span class="number">1</span> - AL), (<span class="number">1</span> - Y).T)) / (<span class="number">1.0</span> * m)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(cost.shape==())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_activation_backward</span>(<span class="params">dA, cache, activation</span>):</span><br><span class="line">   <span class="comment"># 从缓存中获取线性缓存和激活缓存</span></span><br><span class="line">   linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 根据激活函数选择相应的反向传播函数</span></span><br><span class="line">   <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span>:</span><br><span class="line">       <span class="comment"># 计算ReLU激活函数的梯度</span></span><br><span class="line">       dZ = relu_backward(dA, activation_cache)</span><br><span class="line">       <span class="comment"># 计算线性层的梯度</span></span><br><span class="line">       dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">   <span class="keyword">elif</span> activation == <span class="string">&quot;sigmoid&quot;</span>:</span><br><span class="line">       <span class="comment"># 计算Sigmoid激活函数的梯度</span></span><br><span class="line">       dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">       <span class="comment"># 计算线性层的梯度</span></span><br><span class="line">       dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 返回各参数的梯度</span></span><br><span class="line">   <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="comment">#参数更新</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span><br><span class="line">   <span class="comment"># 计算层数 L（不包括输入层）</span></span><br><span class="line">   L = <span class="built_in">len</span>(parameters) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 遍历每一层</span></span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">       <span class="comment"># 更新权重矩阵 W</span></span><br><span class="line">       parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i)] -= learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i)]</span><br><span class="line">       <span class="comment"># 更新偏置项 b</span></span><br><span class="line">       parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i)] -= learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i)]</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 返回更新后的参数</span></span><br><span class="line">   <span class="keyword">return</span> parameters</span><br><span class="line">        </span><br><span class="line"> </span><br></pre></td></tr></table></figure><h3 id="4-2定义模型参数"><a href="#4-2定义模型参数" class="headerlink" title="4.2定义模型参数"></a>4.2<strong>定义模型参数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="4-3Two-layer-neural-network定义"><a href="#4-3Two-layer-neural-network定义" class="headerlink" title="4.3Two-layer neural network定义"></a>4.3<strong>Two-layer neural network定义</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">two_layer_model</span>(<span class="params">X,Y,layers_dims,learning_rate = <span class="number">0.0075</span>,num_iterations=<span class="number">3000</span>,print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现一个两层神经网络：LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    X -- 输入数据，形状为 (n_x, 样本数量)</span></span><br><span class="line"><span class="string">    Y -- 真实标签向量（猫为 0，非猫为 1），形状为 (1, 样本数量)</span></span><br><span class="line"><span class="string">    layers_dims -- 层的维度 (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- 优化循环的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降的 learning rate</span></span><br><span class="line"><span class="string">    print_cost -- 如果设为 True，将每 100 次迭代打印一次损失函数值</span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    parameters -- 包含 W1, W2, b1, b2 的字典&quot;&quot;&quot;</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x,n_h,n_y) = layers_dims</span><br><span class="line">    parameters = initialize_parameters(n_x,n_h,n_y)</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">    b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">    b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,num_iterations):</span><br><span class="line">        <span class="comment">#前向传播</span></span><br><span class="line">        A1,cache1 = linear_activation_forward(X,parameters[<span class="string">&quot;W1&quot;</span>],parameters[<span class="string">&quot;b1&quot;</span>],activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        A2,cache2 = linear_activation_forward(A1,parameters[<span class="string">&quot;W2&quot;</span>],parameters[<span class="string">&quot;b2&quot;</span>],activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#计算损失</span></span><br><span class="line">        cost = compute_cost(A2,Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#初始化反向传播</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#反向传播：输入“dA2, cache2, cache1”,输出“dA1, dW2, db2; also dA0 (not used), dW1, db1”</span></span><br><span class="line">        dA1,dW2,db2 = linear_activation_backward(dA2,cache2,activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">        dA0,dW1,db1 = linear_activation_backward(dA1,cache1,activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 设置 grads[&#x27;dWl&#x27;] 为 dW1, grads[&#x27;db1&#x27;] 为 db1, grads[&#x27;dW2&#x27;] 为 dW2, grads[&#x27;db2&#x27;] 为 db2</span></span><br><span class="line">        grads[<span class="string">&#x27;dW1&#x27;</span>] = dW1</span><br><span class="line">        grads[<span class="string">&#x27;db1&#x27;</span>] = db1</span><br><span class="line">        grads[<span class="string">&#x27;dW2&#x27;</span>] = dW2</span><br><span class="line">        grads[<span class="string">&#x27;db2&#x27;</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters,grads,learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#从新的参数中获取 W1, b1, W2, b2</span></span><br><span class="line">        W1 = parameters[<span class="string">&quot;W1&quot;</span>]</span><br><span class="line">        b1 = parameters[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">        W2 = parameters[<span class="string">&quot;W2&quot;</span>]</span><br><span class="line">        b2 = parameters[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#每训练100个样本输出一次损失值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Cost after iteration &#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span>(print_cost):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;The final cost = %f&quot;</span> %(cost))</span><br><span class="line">    <span class="comment"># 画出损失函数曲线</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;iterations (per hundreds)&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Learning rate =&quot;</span> + <span class="built_in">str</span>(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="4-4运行该网络进行训练"><a href="#4-4运行该网络进行训练" class="headerlink" title="4.4运行该网络进行训练"></a>4.4<strong>运行该网络进行训练</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/11/30/goOAxnyP7SLzV4G.png" alt="image-20231130110334416"></p><ul><li><strong>接着通过改变不同的学习率来寻找一个最合适的学习率来进行训练</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0005</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0010</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0035</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0075</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0150</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.0750</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br><span class="line">two_layer_model(train_x, train_y, layers_dims=(n_x, n_h, n_y), learning_rate=<span class="number">0.1500</span>, num_iterations=<span class="number">2500</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(datetime.datetime.now())</span><br></pre></td></tr></table></figure><img src="https://s2.loli.net/2023/11/30/XNvGB4kx1EFITYU.png" alt="image-20231130113327369" style="zoom:40%;" /><h3 id="4-5定义预测函数"><a href="#4-5定义预测函数" class="headerlink" title="4.5定义预测函数"></a>4.5<strong>定义预测函数</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X, y, parameters</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    n = <span class="built_in">len</span>(parameters) // <span class="number">2</span></span><br><span class="line">    p = np.zeros((<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    probas, caches = L_model_forward(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(probas.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> probas[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            p[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: &quot;</span> + <span class="built_in">str</span>(np.<span class="built_in">sum</span>(p == y) / (<span class="number">1.0</span> * m)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure><h3 id="4-6执行预测"><a href="#4-6执行预测" class="headerlink" title="4.6执行预测"></a>4.6<strong>执行预测</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p><strong>结果：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br><span class="line">0.72</span><br></pre></td></tr></table></figure><p>​我们可以注意到，在更少的迭代(比如1500次)上运行模型可以在测试集上获得更好的准确性。这被称为“提前停止”。提前停止是防止过拟合的一种方法。</p><p>​该2层神经网络比逻辑回归实现(70%)有更好的性能(72%)。接着我们看看用$L$层模型是否可以做得更好。</p><h2 id="5-L-layer-Neural-Network实现"><a href="#5-L-layer-Neural-Network实现" class="headerlink" title="5. L-layer Neural Network实现"></a>5. <strong>L-layer Neural Network</strong>实现</h2><h3 id="5-1定义基本功能"><a href="#5-1定义基本功能" class="headerlink" title="5.1定义基本功能"></a>5.1定义基本功能</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_parameters_deep</span>(<span class="params">layer_dims</span>):</span><br><span class="line">    np.random.seed(<span class="number">1</span>)  <span class="comment"># 设置随机种子，确保每次运行结果相同</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)  <span class="comment"># 网络层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment"># 注意，这里的标准差是 np.sqrt(layer_dims[i - 1])，而不是固定的0.01</span></span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(i)] = np.random.randn(layer_dims[i], layer_dims[i - <span class="number">1</span>]) / np.sqrt(layer_dims[i - <span class="number">1</span>])</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i)] = np.zeros((layer_dims[i], <span class="number">1</span>))  <span class="comment"># 初始化偏置为0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 确保权重和偏置的维度正确</span></span><br><span class="line">        <span class="keyword">assert</span>((layer_dims[i], layer_dims[i - <span class="number">1</span>]) == parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i)].shape)</span><br><span class="line">        <span class="keyword">assert</span>((layer_dims[i], <span class="number">1</span>) == parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i)].shape)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_forward</span>(<span class="params">X, parameters</span>):</span><br><span class="line">    caches = []</span><br><span class="line">    A = X  <span class="comment"># 初始激活值设置为输入X</span></span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>  <span class="comment"># 网络层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A  <span class="comment"># 上一层的激活值</span></span><br><span class="line">        <span class="comment"># 前向传播，使用ReLU激活函数</span></span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i)], <span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        caches.append(cache)  <span class="comment"># 保存缓存</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 最后一层使用sigmoid激活函数</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(L)], parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(L)], <span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保输出AL的形状正确</span></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">AL, Y</span>):</span><br><span class="line">    m = Y.shape[<span class="number">1</span>]  <span class="comment"># 样本数量</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算成本</span></span><br><span class="line">    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(<span class="number">1</span> - AL), (<span class="number">1</span> - Y).T)) / (m * <span class="number">1.0</span>)</span><br><span class="line">    cost = np.squeeze(cost)  <span class="comment"># 移除单维度条目</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())  <span class="comment"># 确保成本是标量</span></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">L_model_backward</span>(<span class="params">AL, Y, caches</span>):</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = <span class="built_in">len</span>(caches)  <span class="comment"># 网络层数</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]  <span class="comment"># 样本数量</span></span><br><span class="line">    Y = Y.reshape(AL.shape)  <span class="comment"># 调整Y的形状与AL相匹配</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    dAL = -(np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播的最后一层</span></span><br><span class="line">    current_cache = caches[L - <span class="number">1</span>]</span><br><span class="line">    grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(L)], grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播的其他层</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(L - <span class="number">1</span>)):</span><br><span class="line">        current_cache = caches[i]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(i + <span class="number">2</span>)], current_cache, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        grads[<span class="string">&quot;dA&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i + <span class="number">1</span>)] = db_temp</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_parameters</span>(<span class="params">parameters, grads, learning_rate</span>):</span><br><span class="line">    L = <span class="built_in">len</span>(parameters) // <span class="number">2</span>  <span class="comment"># 网络层数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 使用梯度下降更新参数</span></span><br><span class="line">        parameters[<span class="string">&quot;W&quot;</span> + <span class="built_in">str</span>(i)] -= learning_rate * grads[<span class="string">&quot;dW&quot;</span> + <span class="built_in">str</span>(i)]</span><br><span class="line">        parameters[<span class="string">&quot;b&quot;</span> + <span class="built_in">str</span>(i)] -= learning_rate * grads[<span class="string">&quot;db&quot;</span> + <span class="built_in">str</span>(i)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="5-2定义模型参数"><a href="#5-2定义模型参数" class="headerlink" title="5.2定义模型参数"></a>5.2定义模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br></pre></td></tr></table></figure><h3 id="5-3L-layer-model网络定义"><a href="#5-3L-layer-model网络定义" class="headerlink" title="5.3L_layer_model网络定义"></a>5.3L_layer_model网络定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">L_layer_model</span>(<span class="params">X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    实现一个L层神经网络: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    X -- 数据，形状为 (样本数, num_px * num_px * 3) 的numpy数组</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量（如果是猫则为0，不是猫则为1），形状为 (1, 样本数)</span></span><br><span class="line"><span class="string">    layers_dims -- 包含输入大小和每层大小的列表，长度为 (层数 + 1)</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降更新规则的学习率</span></span><br><span class="line"><span class="string">    num_iterations -- 优化循环的迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- 如果为True，则每100步打印一次成本</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    parameters -- 模型学习到的参数。它们可以用于预测。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)  <span class="comment"># 设置随机种子以保持结果的一致性</span></span><br><span class="line">    costs = []  <span class="comment"># 记录成本</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算成本</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 每100个训练样本打印成本</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> (<span class="string">&quot;迭代次数 %i 后的成本: %f&quot;</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 绘制成本曲线</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;成本&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;迭代次数（每十个）&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;学习率 = &quot;</span> + <span class="built_in">str</span>(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="5-4运行该网络进行训练"><a href="#5-4运行该网络进行训练" class="headerlink" title="5.4运行该网络进行训练"></a>5.4运行该网络进行训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x,train_y,layers_dims,num_iterations=<span class="number">2500</span>,print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/11/30/iqComW1LQv3KdzO.png" alt="image-20231130112550816"></p><h3 id="5-5执行预测"><a href="#5-5执行预测" class="headerlink" title="5.5执行预测"></a>5.5执行预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 0.9856459330143539</span><br><span class="line">Accuracy: 0.8</span><br></pre></td></tr></table></figure><p>​该网络比4.3提到的网络测试集准确率提高了8%的准确率。</p><p>​我们也可以利用<strong>print_mislabeled_images</strong>函数来看被L_layer网络错误分类的图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/11/30/avnbL49zptZVkhN.png" alt="image-20231130113042197"></p>]]></content>
      
      
      <categories>
          
          <category> 传统算法学习 </category>
          
          <category> 吴恩达课程学习 </category>
          
          <category> Neural Networks and Deep Learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>torch-manual-seed(3407)-is-all-you-need:-On-the-influence-of-random seeds-in-deep-learning-architectures-for computer-vision</title>
      <link href="/2023/10/29/torch-manual-seed-3407-is-all-you-need-On-the-influence-of-random-seeds-in-deep-learning-architectures-for-computer-vision/"/>
      <url>/2023/10/29/torch-manual-seed-3407-is-all-you-need-On-the-influence-of-random-seeds-in-deep-learning-architectures-for-computer-vision/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2023/11/29/NKpeY3FyC5AMUxX.png" alt="摘要"></p><p>​最近阅读到这篇论文，对于里面的调种子的方法比较感兴趣，简单记录一下阅读过程中的一些发现以及一些思考。</p><p>​首先先说结论：<strong>即使方差不是很大，也很容易找到一个比平均值表现好得多或差得多的异常值</strong>。这意味着如果计算量充足并且研究者对于自己的研究足够负责，那么最好探究一下因为随机种子设置、数据集划分等随即来源对于实验结果的影响，最终通过类似平均值、均值、方差、标准差、最值等数据形式展现。</p><span id="more"></span><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>​这是一篇论述通过设置torch.manual seed(3407)随机种子对于模型最终结果影响的实验性文章，作者通过大量实验选择出来一类可以影响模型精度的种子。在CIFAR10数据集上，作者探究了一万多个随机种子，同时也利用预训练模型在ImageNet大数据集上进行实验。</p><p>​整篇文章围绕着这三个问题展开的：</p><ol><li>随机种子的不同导致的模型结果分布是怎样的?</li><li>是否存在黑天鹅事件，也就是存在效果明显不同的随机种子？</li><li>在更大的数据集上进行预训练是否可以减少由选择种子引起的差异性？</li></ol><h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>​由于这只是一个简单的实验，并不是想要得到模型最好的结果，因此作者仅尝试在V100 GPU上进行实验，GPU时间限制为1000小时。（作为实验来讲，算力还是很充足的😄）</p><p>​<font color=Red><strong>CIFAR 10</strong></font>: 在CIFAR 10数据集上，作者选取了10000个随机种子，每个随机种子利用30s的时间来训练和测试，总耗时将近83h。模型架构采用的是9层ResNet，优化器是SGD。为了保证部分实验中，模型是接近收敛的，因此作者对其中500次的结果训练时长延迟至1分钟。最后，总算力消耗为90h的V100 GPU运行时间。</p><p>​<font color=Red><strong>ImageNet</strong></font>: 在ImageNet大型数据集上难以快速进行实验，因此作者使用预训练好的网络，然后仅仅对于最后一层分类层进行初始化并从头训练。每次实验模型训练时间为2h，测试50s。最终总耗时440h的V100 GPU运行时间。</p><p>​<font color=Red>实验缺陷</font>: </p><p>​作者通过实验表面，以上这种实验设置存在如下缺陷：</p><ol><li>实验中，模型结果并不是SOTA（由于运行时间限制导致，可能训练时间进一步增加可以得到更好的结果）</li><li>在ImageNet数据集上，作者使用的是预训练模型，仅对最后一层网络进行训练，所以最终的结果只能说明与最后一层的训练和优化有关（而不是整个模型）。</li></ol><h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><h3 id="1、随机种子的不同导致的模型结果分布是怎样的"><a href="#1、随机种子的不同导致的模型结果分布是怎样的" class="headerlink" title="1、随机种子的不同导致的模型结果分布是怎样的?"></a>1、随机种子的不同导致的模型结果分布是怎样的?</h3><p>​针对第一个问题，作者首先在CIFAR 10上对500个不同的种子进行训练，得到如下结果图:（视线为均值，深红色区域对应一个标准差，浅红色对应最大值和最小值）</p><p><img src="https://s2.loli.net/2023/11/29/19jNdcPgLsGtfDW.png" alt="Resnet9架构在CIFAR 10上的验证精度随训练历元数的变化，实线表示超过500个种子的平均值，深红色区域对应一个标准差，浅红色对应最大值和最小值。"></p><p><strong>作者发现：</strong></p><p>​1. 模型在25个epoch后准确率就不增加了，说明训练收敛了。然而，准确率的标准差并没有收敛（红色区域并没有变小），说明训练更久不会减少随机种子带来的差异。</p><p><img src="https://s2.loli.net/2023/11/29/9ceyYBISJi32lmU.png" alt="在CIFAR 10上对超过500粒种子的Resnet9架构进行最终验证精度的直方图和密度图。底部的每一个破折号对应一次epoch。"></p><ol start="2"><li>模型准确率大多集中在90.5%至90.1%左右，也就是说，如果不去刻意选取特别好或者特别差的随机种子的话，不同随机种子带来的准确率差异普遍为0.5%。(结果图中底部的每个破折号对应一次跑动)</li></ol><p>​因此，回到第一个问题，<font color=Red><strong>随机种子的不同导致的模型效果分布是怎样的？</strong> </font>答案显然就是：不同随机种子的运行效果分布是<strong>相当集中</strong>的——这是一个比较令人满意的结果，除非有人刻意去“调”随机种子，不然最后的结果是能够比较反应模型效果的。</p><h3 id="2、是否存在“黑天鹅事件”？"><a href="#2、是否存在“黑天鹅事件”？" class="headerlink" title="2、是否存在“黑天鹅事件”？"></a>2、是否存在“黑天鹅事件”？</h3><p><img src="https://s2.loli.net/2023/11/29/oIiv8JFYd5UE2As.png" alt="CIFAR 10数据集上，不同随机种子在较长和较短训练时间下的结果"></p><p><strong>作者发现：</strong></p><ol><li>模型训练长比训练时间短效果更好。此外，在短训练时间条件下，准确率最小值和最大值相差为1.82%。（可以说在深度学习领域里差距比较大了）</li><li>10000个随机种子的结果发现，准确率主要位于89.5%至90.5%的区间中。如果不扫描大量的种子，就不可能获得更高或更低的极端值。话虽如此，仅通过扫描前10000个随机种子得到的极值并不能代表该模型正常的结果。</li></ol><p><img src="https://s2.loli.net/2023/11/29/rpuCK5PMGwymjAU.png" alt="CIFAR 10 数据集上10000个随机种子的结果"></p><p>​因此，回到问题二：是否存在黑天鹅事件？显然，黑天鹅是存在的。确实有种子表现得比较好或者比较差，这是一个比较令人担忧的结果，<font color=Red><strong>因为当前深度学习社区内，大多文章都是追求模型效果的，而这种较好的效果可能仅仅是由于随机种子引起的</strong>。</font></p><h3 id="3、在更大的数据集上进行预训练是否可以减少由选择种子引起的差异性？"><a href="#3、在更大的数据集上进行预训练是否可以减少由选择种子引起的差异性？" class="headerlink" title="3、在更大的数据集上进行预训练是否可以减少由选择种子引起的差异性？"></a>3、在更大的数据集上进行预训练是否可以减少由选择种子引起的差异性？</h3><p>​为了验证大规模数据集对于种子造成模型精度的差异性，作者接着在ImageNet数据集进行实验，得到了如下结果：<br><img src="https://s2.loli.net/2023/11/29/9iw1N4CYjRcWxMA.png" alt="ImageNet数据集上不同模型不同随机种子的准确率"></p><p><img src="https://s2.loli.net/2023/11/29/g7rMlNHoyjetB6z.png" alt="预训练好的ResNet50网络在Imagenet上最终验证精度的直方图和密度图。底部的每个破折号对应一次跑动。"></p><p><img src="https://s2.loli.net/2023/11/29/7oH9JXKGtb15vBU.png" alt="自监督预训练的Visual Transformer在Imagenet上的最终验证精度的直方图和密度图。底部的每个破折号对应一次跑动"></p><p><strong>作者发现：</strong></p><ol><li>大数据集的上的结果标准差是比CIFA 10<strong>小得多</strong>的，根据上表还是能够观察到大约0.5%的结果提升——这仅仅是由于随机种子引起的。然而，0.5%的准确率提高在CV领域已经可以算是很明显的提升了。</li><li>ImageNet数据集上的结果与CIFAR 10的分布还是有比较大的差异——这是由于随机种子仅仅选了50个的原因。然而，作者认为，哪怕是测试更多的随机种子，准确率分布的差异也不会达到1%以上。</li></ol><p><img src="https://s2.loli.net/2023/11/29/A4xlzpTdhWutHDB.png" alt="自监督预训练的ResNet50网络在Imagenet上的最终验证精度的直方图和密度图"></p><h3 id="3、在更大的数据集上进行预训练是否能减少由选择种子引起的差异性"><a href="#3、在更大的数据集上进行预训练是否能减少由选择种子引起的差异性" class="headerlink" title="3、在更大的数据集上进行预训练是否能减少由选择种子引起的差异性?"></a>3、<strong>在更大的数据集上进行预训练是否能减少由选择种子引起的差异性?</strong></h3><p>​原文作者写到：<strong>它确实减少了因使用不同种子而产生的变异，但它并不能减轻这种变异。在Imagenet上，我们发现最大和最小精度之间的差异约为0.5 %，这通常被社区认为是该数据集的显著性。</strong>说白了，这个并不是一个符合作者预期的结果，因为目前预训练模型在CV领域里是广泛使用的，哪怕是用的同一个预训练模型，只要你扫描50个随机种子进行测试，你还是能够得到比较好的结果。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>​下次做实验时设置这个可能会提升代码模型的精确度😎😎😎</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">3407</span>)</span><br></pre></td></tr></table></figure><p>​论文地址：<a href="https://arxiv.org/abs/2109.08203">https://arxiv.org/abs/2109.08203</a></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
          <category> 论文精读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉; </tag>
            
            <tag> 深度学习; </tag>
            
            <tag> Tricks; </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于AM-CNN的细菌图谱分类模型</title>
      <link href="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="1-实现过程"><a href="#1-实现过程" class="headerlink" title="1.实现过程"></a>1.实现过程</h1><h2 id="1-1模型选择"><a href="#1-1模型选择" class="headerlink" title="1.1模型选择"></a>1.1模型选择</h2><h3 id="1-1-1基于注意力改进的卷积神经网络算法（AM-CNN）"><a href="#1-1-1基于注意力改进的卷积神经网络算法（AM-CNN）" class="headerlink" title="1.1.1基于注意力改进的卷积神经网络算法（AM-CNN）"></a>1.1.1基于注意力改进的卷积神经网络算法（AM-CNN）</h3><p>​<font color=Red><strong>AM-CNN</strong></font></p><p>​AM-CNN（基于注意力改进的卷积神经网络）模型是一种用于处理细菌拉曼图谱数据的新型深度学习算法。该模型在输入数据特征组合时，考虑了细菌拉曼图谱的波长向量和强度向量，通过滑动窗口方式获取目标词与周围词的综合向量。首先，通过第一次的注意力机制捕获实体与序列中每个词的相关性，并将其与输入的综合词向量矩阵相乘。接着，对卷积结果使用第二次注意力机制捕获视窗与关系的相关性。最终，将卷积结果与相关性矩阵相乘，得到最后的输出结果。</p><span id="more"></span><p>​（这个模型的核心在于将细菌拉曼图谱的波长向量和强度向量与输入数据进行组合。首先，将这两种向量进行拼接，构成了最初的输入向量。接着，使用滑动窗口的方式将目标词与周围词组合在一起，形成综合向量。第一次的注意力机制应用在实体与序列中每个词的相关性。将相关性矩阵与输入的综合词向量矩阵相乘，得到一个二维矩阵。然后，使用卷积提取特征，并对卷积结果使用第二次注意力机制捕获视窗与关系的相关性。最后，将卷积结果与相关性矩阵相乘，得到最终的输出结果。通过这种方式，模型能够充分考虑细菌拉曼图谱的波长向量和强度向量在输入数据中的关联关系。）</p><img src="image-20230801094141414.png" width="50%" height="50%"><p>​模型结构</p><p>网络构建：</p><ol><li>输入层：将细菌拉曼图谱的波长向量和强度向量作为输入数据。波长向量和强度向量可以分别作为两个输入通道。</li><li>注意力机制1：使用注意力机制1捕获输入数据中实体与序列中每个词的相关性。可以采用自注意力（self-attention）机制或全局平均池化（global average pooling）等方式。</li><li>综合词向量矩阵：将注意力机制1得到的相关性矩阵与输入的综合词向量矩阵相乘，得到一个二维矩阵，用于提取特征。</li><li>卷积层：使用卷积层对综合词向量矩阵进行特征提取，可以使用不同的卷积核大小和数量，以捕获不同尺度的特征。</li><li>注意力机制2：使用注意力机制2对卷积结果进行进一步的特征选择，捕获视窗与关系的相关性。</li><li>全连接层：将经过注意力机制2的卷积结果展平，并通过全连接层进行特征融合和映射，得到最终的输出。</li><li>输出层：根据任务需求，可以添加合适的输出层，如softmax层用于分类任务，sigmoid层用于二分类任务等。</li><li>损失函数：选择合适的损失函数用于模型的训练和优化。</li></ol><p>​</p><p>​我们在训练网络时，为了使得模型可以更快更准确的训练，加入了学习率的自适应调整函数，可以根据训练的数据情况以及已有的训练量来自动调整学习率，使训练效果达到最优。</p><p>具体模型构架如下：</p><ol><li>我们首先将训练数据集按照4：1划分成训练集与验证集。</li><li>构建AM-CNN网络框架</li><li>将训练数据输入AM-CNN网络进行1000轮训练</li><li>待模型训练好后，使用测试数据测试模型预测结果</li><li>调整模型参数，待模型结构最优后，测试模型最终的分类准确度，并记录训练期间 Loss 值的变动情况。</li></ol><h2 id="1-2基于注意力改进的卷积神经网络（AM-CNN）实验结果"><a href="#1-2基于注意力改进的卷积神经网络（AM-CNN）实验结果" class="headerlink" title="1.2基于注意力改进的卷积神经网络（AM-CNN）实验结果"></a>1.2基于注意力改进的卷积神经网络（AM-CNN）实验结果</h2><p>训练结束后，本实验分别随机选取了3种细菌的50个拉曼数据进行模型评估。</p><h3 id="1-2-1未标注数据混合"><a href="#1-2-1未标注数据混合" class="headerlink" title="1.2.1未标注数据混合"></a>1.2.1未标注数据混合</h3><h3 id="1-2-2标注数据混合（6种细菌训练与分类效果）"><a href="#1-2-2标注数据混合（6种细菌训练与分类效果）" class="headerlink" title="1.2.2标注数据混合（6种细菌训练与分类效果）"></a>1.2.2标注数据混合（6种细菌训练与分类效果）</h3><h5 id="1-训练准确率变化情况"><a href="#1-训练准确率变化情况" class="headerlink" title="1.训练准确率变化情况"></a>1.训练准确率变化情况</h5><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E7%8E%87+%E8%AF%84%E4%BC%B0%E5%87%86%E7%A1%AE%E7%8E%87.png" alt="训练准确率+评估准确率"></p><p>准确率变化较为理想，满足预期要求!</p><ul><li>在第38次训练后模型的<strong>训练准确率</strong>维持在98%</li><li>在第38次训练后模型的<strong>验证准确率</strong>维持在95%</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">模型验证通常是在训练过程中使用一个独立于训练集和测试集的数据集进行模型性能评估。它可以用来检测模型是否过拟合或者欠拟合。如果模型在训练集上表现良好，但在验证集上表现较差，那就意味着模型可能过拟合了。这种情况下，可以采取一些方法如提前停止训练或增加正则化等来防止模型过拟合。</span><br><span class="line"></span><br><span class="line">训练准确率代表模型在当前训练数据上的表现。训练多轮后，训练准确率会逐渐提高，这表明模型学到了更多的数据分类特征。但是，如果训练准确率开始变得非常高，而验证准确率却不再提高，这说明模型开始过拟合训练数据。</span><br></pre></td></tr></table></figure><h5 id="2-训练LOSS值变化情况"><a href="#2-训练LOSS值变化情况" class="headerlink" title="2.训练LOSS值变化情况"></a>2.训练LOSS值变化情况</h5><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/1LOSS.png" alt="1LOSS"></p><h5 id="模型分别对于n种细菌数据各自分类情况"><a href="#模型分别对于n种细菌数据各自分类情况" class="headerlink" title="模型分别对于n种细菌数据各自分类情况"></a>模型分别对于n种细菌数据各自分类情况</h5><p><img src="/2023/10/06/%EF%BF%BD%EF%BF%BDAM-CNN%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD1%06%7B!%EF%BF%BD/98.833%25%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD%06%7B%C5%B5Process.png" alt="98.833%四种细菌分类情况Process"></p><p><img src="/2023/10/06/%EF%BF%BD%EF%BF%BDAM-CNN%EF%BF%BD%EF%BF%BD%EF%BF%BD%EF%BF%BD1%06%7B!%EF%BF%BD/98.75%25%06%7B%C5%B5process.png" alt="98.75%分类情况process"></p><p><strong>（1）.未标注</strong></p><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/FREE.png" alt="FREE"></p><p><strong>（2）.标注</strong></p><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/LABEL.png" alt="LABEL"></p><h5 id="5-模型对于测试集的验证情况"><a href="#5-模型对于测试集的验证情况" class="headerlink" title="5.模型对于测试集的验证情况"></a>5.模型对于测试集的验证情况</h5><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/all_bacteria_heatmap.png" alt="all_bacteria_heatmap"></p><h5 id="6-模型六种细菌的ROC变化情况"><a href="#6-模型六种细菌的ROC变化情况" class="headerlink" title="6.模型六种细菌的ROC变化情况"></a>6.模型六种细菌的ROC变化情况</h5><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/ROC.png" alt="ROC"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROC曲线可以帮助我们了解分类器在不同阈值下的表现情况，以及在不同的分类阈值下分类器的敏感性和特异性。曲线的横坐标是假正率（False Positive Rate）即被错误地分为正类的样本占所有负样本的比例，曲线的纵坐标是真正率（True Positive Rate）即被正确地分为正类的样本占所有正样本的比例，曲线越接近左上角，说明分类器的表现越好。    通过ROC曲线我们可以判断分类器的性能是否足够好，同时也可以比较多个分类器的性能，选出最佳的分类器。    举个例子如果ROC曲线下的面积（AUC）接近于1，则说明分类器的性能较好，如果ROC曲线下的面积接近于0.5，则说明分类器的性能不如随机猜测（随机猜测的AUC为0.5）。</span><br></pre></td></tr></table></figure><p><img src="C:/Users/m/Desktop/新建文件夹/PR.png" alt="PR"></p><h3 id="1-2-3与经典网络相比的准确率提升程度"><a href="#1-2-3与经典网络相比的准确率提升程度" class="headerlink" title="1.2.3与经典网络相比的准确率提升程度"></a>1.2.3与经典网络相比的准确率提升程度</h3><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/compare-169657254906614.png" alt="compare"></p><table><thead><tr><th>Method</th><th>未标注</th><th>标注</th></tr></thead><tbody><tr><td>ITQ</td><td>0.615</td><td>0.628</td></tr><tr><td>SH</td><td>0.684</td><td>0.744</td></tr><tr><td>DSH</td><td>0.765</td><td>0.780</td></tr><tr><td>SpH</td><td>0.795</td><td>0.815</td></tr><tr><td>BGAN</td><td>0.847</td><td>0.913</td></tr><tr><td>AM-CNN</td><td>0.954</td><td>0.978</td></tr></tbody></table><p><img src="/2023/10/06/%E5%9F%BA%E4%BA%8EAM-CNN%E7%9A%84%E7%BB%86%E8%8F%8C%E5%9B%BE%E8%B0%B1%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/performence-169657256481017.png" alt="performence"></p><h2 id="2-创新点"><a href="#2-创新点" class="headerlink" title="2.创新点"></a>2.创新点</h2><p>关于基于细菌拉曼光谱和注意力机制的CNN网络的创新性内容，具有以下几个主要优势：</p><p>1.<strong>引入注意力机制</strong>：</p><p>传统的卷积神经网络在图像分类任务中，通常使用池化层、全局卷积核等方式提取图像特征。而引入了注意力机制之后，可以使网络更加关注细菌图像的重要特征，从而提高分类精度。</p><p>注意力机制在网络中加入一个注意力模块，用于选择和强调输入光谱数据中的重要信息。在该网络中，注意力机制可以结合不同的损失函数进行优化，从而使网络更加有效地学习到重要特征，提高分类的效果。</p><p>2.<strong>应用自适应阈值策略</strong>：</p><p>传统的细菌分类算法基于训练集的特征设定分类阈值，在测试集上运用时分类效果可能会有所下降。而基于细菌拉曼光谱和注意力机制的CNN网络，可以采用自适应阈值策略，实现对数据特征的自适应调整，避免了传统算法阈值设定不准确的问题。</p><p>该网络中引入自适应阈值参数，在网络训练时动态更新自适应阈值<strong>参数</strong>，通过不断的反馈训练数据的特征，不断地调整自适应阈值参数，避免了传统算法阈值设定不准确的问题。</p><p>3.<strong>优化模型参数：</strong></p><p>优化模型参数可以提高网络的训练速度和泛化能力，进而提高分类精度。在基于细菌拉曼光谱和注意力机制的CNN网络中，可以通过改变层数、添加跨层连接等方式优化模型的参数，提高分类的效果。</p><p>我们的该网络可以通过增加网络层数，引入残差连接、shuffle连接等方式优化模型，增强网络的泛化能力。此外，还可以使用自适应学习率、正则化等技术，进一步优化网络参数。</p><p>4.<strong>数据来源新颖</strong>：传统的基于图像的细菌分类方法需要基于显微镜下的图像进行分析和识别。而利用细菌拉曼光谱，则是通过非接触方式直接获取细菌组织的光谱数据，避免了细菌的处理过程对样本造成的影响和污染，同时提供了更加全局、多层次的细菌信息。</p><p>5.<strong>数据处理方式创新</strong>：细菌拉曼光谱数据与图像数据不同，需要考虑光谱数据的高维度、数据噪声等问题。基于此，我们运用RamanSpectra库对数据进行预处理和降维，提取有用的特征信息，有利于构建更加高效的分类模型。</p><h4 id="部分参考："><a href="#部分参考：" class="headerlink" title="部分参考："></a><strong>部分参考：</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.王吉俐,彭敦陆,陈章,等. AM-CNN:一种基于注意力的卷积神经网络文本分类模型[J]. 小型微型计算机系统,2019,40(4):710-714. DOI:10.3969/j.issn.1000-1220.2019.04.004.</span><br><span class="line"></span><br><span class="line">2.Wang, Linlin, et al. &quot;Relation classification via multi-level attention cnns.&quot; Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2016.</span><br><span class="line"></span><br><span class="line">3.https://mp.weixin.qq.com/s/N-lSzF72TooXAil5FUUW3w</span><br></pre></td></tr></table></figure><h2 id="2-模型部署"><a href="#2-模型部署" class="headerlink" title="2.模型部署"></a>2.模型部署</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pretty_confusion_matrix <span class="keyword">import</span> pp_matrix_from_data</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve, auc</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve, average_precision_score</span><br><span class="line"><span class="comment">#字体路径</span></span><br><span class="line"><span class="comment"># font = FontProperties(fname=&#x27;./font/songti.ttf&#x27;, size=12)</span></span><br><span class="line"><span class="comment"># plt.rcParams[&#x27;font.sans-serif&#x27;] = [font.get_name()]</span></span><br><span class="line"><span class="comment"># plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False</span></span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&#x27;TF_ENABLE_ONEDNN_OPTS&#x27;</span>] = <span class="string">&#x27;0&#x27;</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span></span><br><span class="line"><span class="comment">#支持中文</span></span><br><span class="line">matplotlib.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]</span><br><span class="line"><span class="comment"># 设置文件夹目录</span></span><br><span class="line">train_dir = <span class="string">&#x27;./Final_Data_Ori/train&#x27;</span>  <span class="comment"># 训练数据文件夹</span></span><br><span class="line">test_dir = <span class="string">&#x27;./Final_Data_Ori/test&#x27;</span>  <span class="comment"># 测试数据文件夹</span></span><br><span class="line">model_dir = <span class="string">&#x27;./model&#x27;</span>  <span class="comment"># 模型保存文件夹</span></span><br><span class="line">batch_size=<span class="number">1</span></span><br><span class="line"><span class="comment">#自动获取所有的细菌标签</span></span><br><span class="line">origin_folder_path=<span class="string">&#x27;./Origin_Data/data2&#x27;</span></span><br><span class="line">labels=[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试数据函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">data_dir</span>):</span><br><span class="line">    X = []</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(data_dir):</span><br><span class="line">        <span class="keyword">if</span> filename.endswith(<span class="string">&quot;.txt&quot;</span>):</span><br><span class="line">            file_path = os.path.join(data_dir, filename)</span><br><span class="line">            data = np.loadtxt(file_path)</span><br><span class="line">            <span class="keyword">if</span> data.ndim &lt; <span class="number">2</span>:</span><br><span class="line">                data = np.expand_dims(data, axis=<span class="number">0</span>)</span><br><span class="line">            X.append(data)</span><br><span class="line">            label = filename.split(<span class="string">&quot;_&quot;</span>)[<span class="number">0</span>]</span><br><span class="line">            y.append(label)</span><br><span class="line">    <span class="keyword">return</span> np.array(X), np.array(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历标签目录</span></span><br><span class="line"><span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(origin_folder_path):</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">dir</span> <span class="keyword">in</span> dirs:</span><br><span class="line">        <span class="comment"># 将文件夹名称添加到标签列表中</span></span><br><span class="line">        labels.append(<span class="built_in">dir</span>)</span><br><span class="line">labels = [label <span class="keyword">for</span> label <span class="keyword">in</span> labels <span class="keyword">if</span> label != <span class="string">&#x27;.ipynb_checkpoints&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(labels)</span><br><span class="line"></span><br><span class="line">X_train, y_train = load_data(train_dir)</span><br><span class="line">X_test, y_test = load_data(test_dir)</span><br><span class="line"><span class="comment">#############################</span></span><br><span class="line"><span class="comment"># 将标签编码为整数</span></span><br><span class="line">unique_labels = np.unique(y_train)</span><br><span class="line">label_dict = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(unique_labels)&#125;</span><br><span class="line">y_train = np.array([label_dict[label] <span class="keyword">for</span> label <span class="keyword">in</span> y_train])</span><br><span class="line">y_test = np.array([label_dict[label] <span class="keyword">for</span> label <span class="keyword">in</span> y_test])</span><br><span class="line">num_classes = <span class="built_in">len</span>(unique_labels)</span><br><span class="line"><span class="comment"># 划分训练集和验证集</span></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型85%model.h5</span></span><br><span class="line"><span class="comment"># best_model_path = os.path.join(model_dir, &#x27;best_model.h5&#x27;)</span></span><br><span class="line">best_model_path = os.path.join(model_dir, <span class="string">&#x27;96%.h5&#x27;</span>)</span><br><span class="line">model = load_model(best_model_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">y_pred_classes = np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test Loss:&quot;</span>, loss)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test Accuracy:&quot;</span>, accuracy)</span><br><span class="line"></span><br><span class="line">cmap = <span class="string">&quot;PuRd&quot;</span></span><br><span class="line">pp_matrix_from_data(y_test, y_pred_classes,columns=labels,lw=accuracy,cmap=cmap)</span><br><span class="line"><span class="built_in">print</span>(y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----------------------------------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(y_pred_classes)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----------------------------------------&#x27;</span>)</span><br><span class="line"><span class="comment"># print(labels)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;----------------------------------------&#x27;</span>)</span><br><span class="line"><span class="comment">##########ROC曲线##############</span></span><br><span class="line"><span class="comment"># 画ROC曲线</span></span><br><span class="line"><span class="comment"># 分别绘制每个类别的ROC曲线</span></span><br><span class="line">fpr = <span class="built_in">dict</span>()</span><br><span class="line">tpr = <span class="built_in">dict</span>()</span><br><span class="line">roc_auc = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(y_test==i, y_pred[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加颜色和标签</span></span><br><span class="line">colors = cycle([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;k&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> i, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(num_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=<span class="number">2</span>,</span><br><span class="line">             label=<span class="string">&#x27;ROC curve of &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span></span><br><span class="line">             <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(labels[i], roc_auc[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一些ROC指令</span></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">&#x27;k--&#x27;</span>, lw=<span class="number">2</span>)</span><br><span class="line">plt.xlim([-<span class="number">0.05</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;ROC Curves&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个类别的平均精度分数</span></span><br><span class="line">average_precision = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">    average_precision[i] = average_precision_score(y_test == i, y_pred[:, i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每个类别的精度-召回率曲线</span></span><br><span class="line">precision = <span class="built_in">dict</span>()</span><br><span class="line">recall = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">    precision[i], recall[i], _ = precision_recall_curve(y_test == i, y_pred[:, i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制每个类别的精度-召回率曲线</span></span><br><span class="line">colors = cycle([<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;k&#x27;</span>])</span><br><span class="line"><span class="keyword">for</span> i, color <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(num_classes), colors):</span><br><span class="line">    plt.plot(recall[i], precision[i], color=color, lw=<span class="number">2</span>,</span><br><span class="line">             label=<span class="string">&#x27;Precision-Recall curve of &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span></span><br><span class="line">             <span class="string">&#x27;&#x27;</span>.<span class="built_in">format</span>(labels[i], average_precision[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一些PR曲线指令</span></span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Recall&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Precision&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Precision-Recall Curves&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&quot;lower right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
          <category> 课外项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习; </tag>
            
            <tag> 自然语言处理; </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
